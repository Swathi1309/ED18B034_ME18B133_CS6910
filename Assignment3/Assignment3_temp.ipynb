{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swathi1309/ED18B034_ME18B133_CS6910/blob/main/Assignment3/Assignment3_temp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERdcRt08YczS"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppVny72Q-SPg"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout, TimeDistributed, Lambda, Activation, Reshape,\\\n",
        "Softmax, Multiply, AdditiveAttention, Concatenate, Add, RepeatVector\n",
        "import tarfile\n",
        "import random\n",
        "from keras.optimizers import RMSprop, Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from math import log\n",
        "from numpy import array, argmax\n",
        "import sklearn\n",
        "\n",
        "import pprint"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV6TzDuChLd8"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "wandb.init(project=\"assg3\", entity='cs6910_project')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2LNP-C3PD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a79c492-ea82-4b77-ab9e-13494a2aa878"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEjUih-v4Ad7"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcMrqAD0GVTJ"
      },
      "source": [
        "File extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1gW8TTyr7x"
      },
      "source": [
        "my_tar = tarfile.open('/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0.tar')\n",
        "my_tar.extractall('/content/drive/MyDrive/Dakshina dataset') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pn0kolbGcbE"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYd7sMK4s8_"
      },
      "source": [
        "import csv\n",
        "hi_to_eng = open(\"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "read_tsv = csv.reader(hi_to_eng, delimiter=\"\\t\")\n",
        "i=0\n",
        "for row in read_tsv:\n",
        "  i +=1\n",
        "  print(row)\n",
        "  if i==30:\n",
        "    break\n",
        "hi_to_eng.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZPMhq4O4F78"
      },
      "source": [
        "# Loading datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DWRiV13NFdC"
      },
      "source": [
        "def load_data(data_path, data):\n",
        "  input_texts=[]\n",
        "  target_texts = []\n",
        "  global tk_input, tk_target, num_input_tokens, num_target_tokens\n",
        "  \n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[: (len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "  \n",
        "  # random.seed(10)\n",
        "  # random.shuffle(input_texts)\n",
        "  # random.seed(10)\n",
        "  # random.shuffle(target_texts)\n",
        "     \n",
        "  if (data == \"train\"):\n",
        "    tk_input = keras.preprocessing.text.Tokenizer(num_words=None, char_level=True)\n",
        "    tk_input.fit_on_texts(input_texts)\n",
        "    enc_input = tk_input.texts_to_sequences(input_texts)\n",
        "    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, padding='post')\n",
        "\n",
        "    tk_target = keras.preprocessing.text.Tokenizer(num_words=None, char_level=True)\n",
        "    tk_target.fit_on_texts(target_texts)\n",
        "    dec_target = tk_target.texts_to_sequences(target_texts)\n",
        "    dec_target = tf.keras.preprocessing.sequence.pad_sequences(dec_target, padding='post')\n",
        "\n",
        "    dec_input = np.zeros(dec_target.shape)\n",
        "    dec_input[:,:(dec_target.shape[1]-1)] = dec_target[:,1:]\n",
        "  \n",
        "  else:\n",
        "    enc_input = tk_input.texts_to_sequences(input_texts)\n",
        "    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, padding='post', maxlen = max_input_length)\n",
        "\n",
        "    dec_target = tk_target.texts_to_sequences(target_texts)\n",
        "    dec_target = tf.keras.preprocessing.sequence.pad_sequences(dec_target, padding='post', maxlen = max_target_length)\n",
        "\n",
        "    dec_input = np.zeros(dec_target.shape)\n",
        "    dec_input[:,:(dec_target.shape[1]-1)] = dec_target[:,1:]\n",
        "  \n",
        "  return enc_input, dec_target, dec_input"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xJb4hbNNPO-"
      },
      "source": [
        "data_path_train = \"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "data_path_dev = '/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv'\n",
        "data_path_test = '/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv'\n",
        "\n",
        "global enc_input_train, dec_input_train, dec_target_train, enc_input_dev, dec_input_dev, dec_target_dev\n",
        "global tk_input, tk_target, reverse_input_char_index, reverse_target_char_index\n",
        "global input_token_train, output_token_train, max_input_length, max_target_length\n",
        "global y_true_dev, y_true_train\n",
        "global classes\n",
        "\n",
        "# Loading datasets\n",
        "enc_input_train, dec_input_train, dec_target_train = load_data(data_path_train,'train')\n",
        "\n",
        "max_input_length = enc_input_train.shape[1]\n",
        "max_target_length = dec_target_train.shape[1]\n",
        "\n",
        "num_input_tokens = len(tk_input.word_index)+1\n",
        "num_target_tokens = len(tk_target.word_index)+1\n",
        "\n",
        "enc_input_dev, dec_input_dev, dec_target_dev = load_data(data_path_dev,'dev')\n",
        "enc_input_test, dec_input_test, dec_target_test = load_data(data_path_test,'test')\n",
        "\n",
        "# One hot encoding of target outputs\n",
        "y_true_dev = np.zeros((dec_target_dev.shape[0],dec_target_dev.shape[1], num_target_tokens))\n",
        "for i in range(0,dec_target_dev.shape[0]):\n",
        "  y_true_dev[i] = to_categorical(dec_target_dev[i], num_classes = num_target_tokens)\n",
        "\n",
        "y_true_train = np.zeros((dec_target_train.shape[0],dec_target_train.shape[1], num_target_tokens))\n",
        "for i in range(0,dec_target_train.shape[0]):\n",
        "  y_true_train[i] = to_categorical(dec_target_train[i], num_classes = num_target_tokens)\n",
        "\n",
        "# Integer to character dictionary\n",
        "reverse_input_char_index = dict((i, char) for char, i in tk_input.word_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in tk_target.word_index.items())\n",
        "\n",
        "# Defining classes\n",
        "classes = [\"0\"]\n",
        "for i in range(1,29):\n",
        "  classes.append(reverse_target_char_index[i])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JFCpjQw8WuD"
      },
      "source": [
        "# Defining functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBsU_wpbCkSH"
      },
      "source": [
        "Training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHFjZGFxpCb"
      },
      "source": [
        "# Encoder\n",
        "def encoder(enc_out,enc_block, latent_dim, enc_layers):\n",
        "  states = []\n",
        "  for i in range(0,enc_layers):\n",
        "    if enc_block =='LSTM':\n",
        "      enc_out ,h ,c = LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      s = [h,c]\n",
        "      states.append(s)\n",
        "    elif enc_block =='GRU':\n",
        "      enc_out ,s = GRU(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      states.append(s)\n",
        "    else:\n",
        "      enc_out ,s = SimpleRNN(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      states.append(s)\n",
        "    \n",
        "  return enc_out,states\n",
        "\n",
        "# Decoder\n",
        "def decoder(X_input_dec, hs_init, latent_dim, dec_block, dec_layers, dense_no, drop_no):\n",
        "  out_dec = X_input_dec\n",
        "  hs = hs_init\n",
        "  for i in range (0,dec_layers):\n",
        "    if dec_block == 'LSTM':\n",
        "      out_dec, h, s = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "    elif dec_block == 'GRU':\n",
        "      out_dec, _ = GRU(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "    else:\n",
        "      out_dec, _ = SimpleRNN(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "  \n",
        "  if dense_no != 0:\n",
        "    out_dec = TimeDistributed(Dense(dense_no, activation=\"relu\", name = 'output'), name = 'dense_hidden')(out_dec)\n",
        "    out_dec = Dropout(drop_no)(out_dec)\n",
        "  out_dec = TimeDistributed(Dense(num_target_tokens, activation=\"softmax\", name = 'output'), name = 'dense')(out_dec)\n",
        "  \n",
        "  return out_dec\n",
        "\n",
        "# Bahnadau Attention\n",
        "def b_attention(latent_dim):\n",
        "  query_in = keras.Input((latent_dim,))\n",
        "  values_in = keras.Input((max_input_length,latent_dim))\n",
        "  query = Reshape((1,latent_dim), name='query_reshape')(query_in)\n",
        "  score_unit2 = Dense(num_target_tokens, name='score_unitq')(query)\n",
        "  score_unit1 = Dense(num_target_tokens, name='score_unitv')(values_in)\n",
        "  score_unit12 = Add()([score_unit1, score_unit2])\n",
        "  score_unit12 = Activation('tanh')(score_unit12)\n",
        "  score = Dense(1)(score_unit12)\n",
        "  attention_weights = Softmax(axis=1)(score)\n",
        "  context_vector = Multiply()([attention_weights,values_in])\n",
        "  context_vector = Lambda(lambda x: tf.reduce_sum(x,axis=1))(context_vector)\n",
        "  context_vector = RepeatVector(max_target_length)(context_vector)\n",
        "\n",
        "  model = Model(inputs = [query_in,values_in], outputs = [context_vector, attention_weights], name='attention')\n",
        "  return model\n",
        "\n",
        "# Final RNN model for training\n",
        "def RNN_model(embedding_no, enc_block, latent_dim, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "  X_input_enc = keras.Input(shape=(max_input_length,))\n",
        "  X_input_dec = keras.Input(shape=(max_target_length,))\n",
        "  X_enc = tf.keras.layers.Embedding(num_input_tokens, embedding_no, name = 'embedding_enc')(X_input_enc)\n",
        "  X_dec = tf.keras.layers.Embedding(num_target_tokens, embedding_no, name = 'embedding_dec')(X_input_dec)\n",
        "  _,s_init = encoder(X_enc,enc_block, latent_dim, enc_layers)\n",
        "  out_dec = decoder(X_dec,s_init, latent_dim, dec_block, dec_layers, dense_no, drop_no)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "  return model\n",
        "\n",
        "# Final RNN model with attention for training\n",
        "def RNN_w_att(embedding_no, enc_block, latent_dim, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "  X_input_enc = keras.Input(shape=(max_input_length,))\n",
        "  X_input_dec = keras.Input(shape=(max_target_length,))\n",
        "\n",
        "  X_enc = tf.keras.layers.Embedding(num_input_tokens, embedding_no, name = 'embedding_enc')(X_input_enc)\n",
        "  X_dec = tf.keras.layers.Embedding(num_target_tokens, embedding_no, name = 'embedding_dec')(X_input_dec)\n",
        "\n",
        "  values,s_init = encoder(X_enc,enc_block, latent_dim, enc_layers)\n",
        "  context_vector, weights = b_attention(latent_dim)([s_init[len(s_init)-1][0], values])\n",
        "  X_dec = Concatenate(name = 'concatenate')([context_vector,X_dec])\n",
        "\n",
        "  out_dec = decoder(X_dec,s_init, latent_dim, dec_block, dec_layers, dense_no, drop_no)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JuwWrzxDE93"
      },
      "source": [
        "Inference models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oquP5kRhmm8H"
      },
      "source": [
        "# Encoder model for inference\n",
        "def enc_inference(model,enc_layers,rnn_block):\n",
        "  X_enc = model.input[0]\n",
        "  X_in_enc = model.get_layer('embedding_enc')(X_enc)\n",
        "  enc_outputs = X_in_enc\n",
        "  states = []\n",
        "  if rnn_block == 'LSTM':\n",
        "    for i in range(0,enc_layers):\n",
        "      enc_outputs, h_enc, s_enc = model.get_layer('encoder'+str(i))(enc_outputs)\n",
        "      states.append([h_enc, s_enc])\n",
        "  else:\n",
        "    for i in range(0,enc_layers):\n",
        "      enc_outputs, s_enc = model.get_layer('encoder'+str(i))(enc_outputs)\n",
        "      states.append(s_enc)\n",
        "  \n",
        "  enc_model = keras.Model(inputs = X_enc, outputs = [enc_outputs, states])\n",
        "  return enc_model\n",
        "\n",
        "# Decoder model for inference\n",
        "def dec_inference(model, latent_dim, dec_layers, rnn_block, attn, embedding_dim):\n",
        "  if attn==True:\n",
        "    dec_inputs = keras.Input(shape=(None, latent_dim+embedding_dim))\n",
        "    dec_outputs = dec_inputs\n",
        "  else:\n",
        "    dec_inputs = model.input[1]\n",
        "    dec_outputs = model.get_layer('embedding_dec')(dec_inputs)\n",
        "  hs_inputs = []\n",
        "  \n",
        "  if rnn_block == 'LSTM':\n",
        "    for i in range(0,dec_layers):\n",
        "      h = keras.Input(shape=(latent_dim,))\n",
        "      c = keras.Input(shape=(latent_dim,))\n",
        "      hs_inputs.append([h,c])\n",
        "    hs_outputs = []\n",
        "    for i in range(0,dec_layers):\n",
        "      dec_outputs, h_dec, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state = hs_inputs[i])\n",
        "      hs_outputs.append([h_dec,s_dec])\n",
        "\n",
        "  else:\n",
        "    for i in range(0,dec_layers):\n",
        "      c = keras.Input(shape=(latent_dim,))\n",
        "      hs_inputs.append(c)\n",
        "    hs_outputs = []\n",
        "    for i in range(0,dec_layers):\n",
        "      dec_outputs, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state = hs_inputs[i])\n",
        "      hs_outputs.append(s_dec)\n",
        "\n",
        "  dec_dense = model.get_layer('dense')\n",
        "  dec_outputs = dec_dense(dec_outputs)\n",
        "  dec_model = keras.Model(inputs = [dec_inputs, hs_inputs], outputs = [dec_outputs, hs_outputs])  \n",
        "  return dec_model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4In3RXxGIaxT"
      },
      "source": [
        "Decoder functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPN10ST5IY15"
      },
      "source": [
        "# Greedy decoder\n",
        "def greedy_decoder(enc_inf, dec_inf, input_seq, rnn_block):\n",
        "  _, states = enc_inf.predict(input_seq)\n",
        "  target_seq = np.zeros((input_seq.shape[0],1))\n",
        "  target_seq[:,0] = tk_target.word_index[\"\\t\"]\n",
        "  pred = [[tk_target.word_index[\"\\t\"]] for _ in range (input_seq.shape[0])]\n",
        "  \n",
        "  if rnn_block == 'LSTM':\n",
        "    for i in range(max_target_length):\n",
        "      output_tokens, state_final = dec_inf.predict([target_seq, states])\n",
        "      x = np.argmax(output_tokens, axis=-1)\n",
        "      target_seq = x\n",
        "      for j in range(input_seq.shape[0]):\n",
        "        pred[j].append(target_seq[j][0])\n",
        "      states = state_final\n",
        "\n",
        "  else:\n",
        "    for i in range(max_target_length):\n",
        "      output_tokens, c = dec_inf.predict([target_seq, states])\n",
        "      x = np.argmax(output_tokens, axis=-1)\n",
        "      target_seq = x\n",
        "      for j in range(input_seq.shape[0]):\n",
        "        pred[j].append(target_seq[j][0])\n",
        "      states = c\n",
        "\n",
        "  return pred\n",
        "\n",
        "# Beam search function\n",
        "def beam_search(enc_inf, dec_inf, input_seq, k):\n",
        "  _, states = enc_inf.predict(input_seq)\n",
        "  target_seq = np.zeros((k,1))\n",
        "  target_seq[:,0] = tk_target.word_index[\"\\t\"]\n",
        "  pred = [[[tk_target.word_index[\"\\t\"]], 0.0] for _ in range (k)]\n",
        "\n",
        "  for i in range(max_target_length):\n",
        "    all_candidates = list()\n",
        "    output_tokens, state_final = dec_inf.predict([target_seq, states])\n",
        "    states = state_final\n",
        "    for j in range(len(pred)):\n",
        "      seq, score = pred[j]\n",
        "      for l in range(output_tokens.shape[-1]):\n",
        "        candidate = [seq + [l], score - log(output_tokens[j][0][l])]\n",
        "        all_candidates.append(candidate)\n",
        "      if i==0:\n",
        "        break\n",
        "    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    pred = ordered[:k]\n",
        "    for j in range (k):\n",
        "      target_seq[j] = pred[j][0][i+1]\n",
        "\n",
        "  final_pred = ordered[:1]\n",
        "  return (final_pred[0][0])\n",
        "\n",
        "# Beam search decoder\n",
        "def beam_search_decoder(enc_inf, dec_inf, input_seq, k):\n",
        "  pred = []\n",
        "  for i in range(input_seq.shape[0]):\n",
        "    input = input_seq[i:i+1,:]\n",
        "    input = np.concatenate((input,)*k, axis = 0)\n",
        "    pred.append(beam_search(enc_inf, dec_inf, input, k))\n",
        "    if i%100 == 0:\n",
        "      print((i/input_seq.shape[0])*100)\n",
        "  return pred"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-MIXTexI0QQ"
      },
      "source": [
        "Word accuracy metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHGzLag7yzoc"
      },
      "source": [
        "def word_accuracy(true_words, pred_words):\n",
        "  count = len([i for i, j in zip(true_words, pred_words) if i==j])\n",
        "  return count/len(pred_words)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ1YkgLoDopr"
      },
      "source": [
        "Encoding to Word conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2zgV0O2iPTo"
      },
      "source": [
        "def transliteration(inputs):\n",
        "  start = tk_target.word_index[\"\\t\"]\n",
        "  end = 0\n",
        "  words = []\n",
        "  for i in range(len(inputs)):\n",
        "    start_index = inputs[i].index(start)\n",
        "    try:\n",
        "      end_index = inputs[i].index(end)\n",
        "      temp = inputs[i][start_index+1:end_index]\n",
        "    except ValueError:\n",
        "      temp = inputs[i][start_index+1:]\n",
        "    word = (list(map(reverse_target_char_index.__getitem__, temp)))\n",
        "    if (word[-1]==\"\\n\"):\n",
        "      word.pop(-1)\n",
        "    separator = ''\n",
        "    word_new = separator.join(word)\n",
        "    words.append(word_new)\n",
        "  return words"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpWZK4G8dvA"
      },
      "source": [
        "# Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq0DRwp1W7_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7477a50b-8178-4236-e608-ca5b46a2dc57"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }\n",
        "\n",
        "parameters_dict = {\n",
        "    'embedding_no':{\n",
        "        'values': [512]\n",
        "      },\n",
        "    'rnn_block':{\n",
        "        'values': ['LSTM']\n",
        "      },\n",
        "    'no_of_layers': {\n",
        "        'values': [2]\n",
        "      },\n",
        "    'latent_dimension': {\n",
        "        'values': [512]\n",
        "      },\n",
        "    'hidden_layer': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'dropout': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'beam_search': {\n",
        "        'values': [4, 2]\n",
        "      },\n",
        "    'epochs' : {\n",
        "        'values' : [20]\n",
        "      },\n",
        "    'optimizer' : {\n",
        "        'values' : ['rmsprop']\n",
        "      },\n",
        "    'learning_rate' : {\n",
        "        'values' : [1e-3]\n",
        "      },\n",
        "    'attention' : {\n",
        "        'values' : [False]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "pprint.pprint(sweep_config)\n",
        "\n",
        "def training_sweep(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "        if config.attention == True:\n",
        "          model = RNN_w_att(config.embedding_no, config.rnn_block, config.latent_dimension, config.rnn_block,\n",
        "                            config.hidden_layer, config.dropout, config.no_of_layers, config.no_of_layers)\n",
        "        else:\n",
        "          model = RNN_model(config.embedding_no, config.rnn_block, config.latent_dimension, config.rnn_block,\n",
        "                            config.hidden_layer, config.dropout, config.no_of_layers, config.no_of_layers)\n",
        "        \n",
        "        if config.optimizer == 'adam':\n",
        "          opt = Adam(config.learning_rate)\n",
        "        else:\n",
        "          opt = RMSprop(config.learning_rate)\n",
        "        model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        model.fit([enc_input_train,dec_input_train], y_true_train, \n",
        "                            epochs=config.epochs,\n",
        "                            validation_data = ([enc_input_dev,dec_input_dev], y_true_dev),\n",
        "                            callbacks = [WandbCallback()]\n",
        "                            )\n",
        "\n",
        "        name = str(config.rnn_block) + '_' + str(config.no_of_layers) + '_' + str(config.embedding_no) + '_' + str(config.latent_dimension) + '_' + str(config.hidden_layer) + '_' + str(config.epochs)\n",
        "        location = '/content/drive/MyDrive/Transliteration/' + name\n",
        "        model.save(location)\n",
        "\n",
        "        enc_inf = enc_inference(model, config.no_of_layers, config.rnn_block)\n",
        "        dec_inf = dec_inference(model, config.latent_dimension, config.no_of_layers, config.rnn_block, config.attention, config.embedding_no)\n",
        "\n",
        "        if (config.beam_search!=0):\n",
        "          predictions = beam_search_decoder(enc_inf, dec_inf, enc_input_dev, config.beam_search)\n",
        "        else:\n",
        "          predictions = greedy_decoder(enc_inf, dec_inf, enc_input_dev, config.rnn_block)\n",
        "\n",
        "          # word_acc = word_accuracy(dec_target_dev,np.array(predictions))\n",
        "          # word_acc = sklearn.metrics.accuracy_score(dec_target_dev,np.array(predictions))\n",
        "          pred_words = transliteration(predictions)\n",
        "\n",
        "          true = list(list(dec_input_dev[i]) for i in range(dec_input_dev.shape[0]))\n",
        "          true_words = transliteration(list(true))\n",
        "          word_acc = word_accuracy(true_words, pred_words)\n",
        "          wandb.log({'word_level_accuracy': word_acc})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'method': 'grid',\n",
            " 'parameters': {'attention': {'values': [False]},\n",
            "                'beam_search': {'values': [4, 2]},\n",
            "                'dropout': {'values': [0]},\n",
            "                'embedding_no': {'values': [512]},\n",
            "                'epochs': {'values': [20]},\n",
            "                'hidden_layer': {'values': [0]},\n",
            "                'latent_dimension': {'values': [512]},\n",
            "                'learning_rate': {'values': [0.001]},\n",
            "                'no_of_layers': {'values': [2]},\n",
            "                'optimizer': {'values': ['rmsprop']},\n",
            "                'rnn_block': {'values': ['LSTM']}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1shGb7ZrJqH"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"assg3\")\n",
        "wandb.agent(sweep_id, training_sweep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39j_dgjQGZ-g"
      },
      "source": [
        "# Plotting confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB9UTuIDpx32"
      },
      "source": [
        "def labels(true_words, pred_words):\n",
        "  true = []\n",
        "  pred = []\n",
        "  for i in range(len(true_words)):\n",
        "    word1 = true_words[i]\n",
        "    word2 = pred_words[i]\n",
        "    len1 = len(word1)\n",
        "    len2 = len(word2)\n",
        "    \n",
        "    for letter1 in word1:\n",
        "      true.append(tk_target.word_index[letter1])\n",
        "    for letter2 in word2:\n",
        "      pred.append(tk_target.word_index[letter2])\n",
        "    \n",
        "    if len1<len2:\n",
        "      for i in range(len2-len1):\n",
        "        true.append(0)\n",
        "    if len1>len2:\n",
        "      for i in range(len1-len2):\n",
        "        pred.append(0)\n",
        "\n",
        "  return true, pred\n",
        "\n",
        "def confusion_matrix(true, pred):\n",
        "  wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None, y_true=true, preds=pred, class_names=classes)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrQtJipmFVXl"
      },
      "source": [
        "true, pred = labels(true_words, pred_words)\n",
        "confusion_matrix(true,pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzptGDXSJPIq"
      },
      "source": [
        "# Rest of the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVWI_VT_5WnP"
      },
      "source": [
        "rnnmodel= RNN_w_att(128,'LSTM',256,'LSTM',0,0,1,1)\n",
        "keras.utils.plot_model(rnnmodel,show_shapes=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHi3CPzn4wu9"
      },
      "source": [
        "att_model = b_attention(256)\n",
        "keras.utils.plot_model(att_model,show_shapes=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nddHQjK_zCJ1"
      },
      "source": [
        "model2 = RNN_model(256, 'LSTM', 512, 'LSTM', 0, 0, 1, 1)\n",
        "keras.utils.plot_model(model2,show_shapes=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K0_aZnYxBPA"
      },
      "source": [
        "model.compile(optimizer=Adam(1e-2), loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "#model.fit([enc_input_train,dec_input_train], y_true_train, \n",
        "        #  batch_size = 128,epochs=20, validation_data=([enc_input_dev,dec_input_dev], y_true_dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-TXM2HpA2HO"
      },
      "source": [
        "enc_inf = enc_inference(model, 1, \"LSTM\")\n",
        "dec_inf = dec_inference(model, 512, 1, 'LSTM')\n",
        "\n",
        "predictions = greedy_decoder(enc_inf, dec_inf, enc_input_dev, 'LSTM')\n",
        "\n",
        "pred_words = transliteration(predictions)\n",
        "\n",
        "print (\"hello\")\n",
        "true = list(list(dec_input_dev[i]) for i in range(dec_input_dev.shape[0]))\n",
        "true_words = transliteration(list(true))\n",
        "word_acc = word_accuracy(true_words, pred_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fwzRhjrEDKD"
      },
      "source": [
        "model = RNN_w_att(256, 'LSTM', 512, 'LSTM', 0, 0, 2, 2)\n",
        "keras.utils.plot_model(model,show_shapes=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSzPg3sL6S0X"
      },
      "source": [
        "enc_inf = enc_inference(model, 2,'LSTM')\n",
        "dec_inf = dec_inference(model, 512, 2, 'LSTM', True, 256)\n",
        "att_inf = model.get_layer('attention')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgn-Pm69dG66"
      },
      "source": [
        "keras.utils.plot_model(dec_inf,show_shapes=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7qprgya8ili"
      },
      "source": [
        "def greedy_decoder_attn(input_seq):\n",
        "  values, s_init = enc_inf.predict(input_seq)\n",
        "  target_seq = np.zeros((input_seq.shape[0],22,1))\n",
        "  target_seq[:,0,:] = tk_target.word_index[\"\\t\"]\n",
        "  pred = [[tk_target.word_index[\"\\t\"]] for _ in range (input_seq.shape[0])]\n",
        "\n",
        "  weights_matrix = []\n",
        "  for i in range(max_target_length):\n",
        "    context_vector, weights = att_inf.predict([s_init[len(s_init)-1][0], values])\n",
        "    weights_matrix.append(weights)\n",
        "    output_tokens = Concatenate()([context_vector, target_seq])\n",
        "    print (output_tokens.shape)\n",
        "    output_tokens, state_final = dec_inf.predict(output_tokens, s_init)\n",
        "    x = np.argmax(output_tokens, axis = -1)\n",
        "    for j in range(input_seq.shape[0]):\n",
        "      target_sequence[j][i]= x[j]\n",
        "      pred[j].append(target_seq[j][0])\n",
        "    s_init = c\n",
        "  return pred, weights_matrix"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aoETlZzdYSw",
        "outputId": "d7ce4089-e4b5-4646-8c3d-6964e313cefe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "predictions, weights = greedy_decoder_attn(enc_input_dev)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4358, 22, 513)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-82cd4e54c5b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgreedy_decoder_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_input_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-b36f23084c9d>\u001b[0m in \u001b[0;36mgreedy_decoder_attn\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_inf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'list'"
          ]
        }
      ]
    }
  ]
}