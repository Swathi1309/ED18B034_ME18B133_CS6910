{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swathi1309/ED18B034_ME18B133_CS6910/blob/main/Assignment3/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppVny72Q-SPg"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout, TimeDistributed\n",
        "import tarfile\n",
        "import random\n",
        "from keras.optimizers import RMSprop, Adam, SGD"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV6TzDuChLd8"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "wandb.init(project=\"CS6910-assg3\", entity=\"narendv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2LNP-C3PD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebf1d1a-28dd-4bbb-ea0c-36db194c3eef"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcMrqAD0GVTJ"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1gW8TTyr7x"
      },
      "source": [
        "my_tar = tarfile.open('/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0.tar')\n",
        "my_tar.extractall('/content/drive/MyDrive/Dakshina dataset') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pn0kolbGcbE"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYd7sMK4s8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53677e02-a832-4cdb-eaca-29f91d2ad216"
      },
      "source": [
        "import csv\n",
        "hi_to_eng = open(\"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "read_tsv = csv.reader(hi_to_eng, delimiter=\"\\t\")\n",
        "i=0\n",
        "for row in read_tsv:\n",
        "  i +=1\n",
        "  print(row)\n",
        "  if i==30:\n",
        "    break\n",
        "hi_to_eng.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['अं', 'an', '3']\n",
            "['अंकगणित', 'ankganit', '3']\n",
            "['अंकल', 'uncle', '4']\n",
            "['अंकुर', 'ankur', '4']\n",
            "['अंकुरण', 'ankuran', '3']\n",
            "['अंकुरित', 'ankurit', '3']\n",
            "['अंकुश', 'aankush', '1']\n",
            "['अंकुश', 'ankush', '3']\n",
            "['अंग', 'ang', '2']\n",
            "['अंग', 'anga', '1']\n",
            "['अंगद', 'agandh', '1']\n",
            "['अंगद', 'angad', '2']\n",
            "['अंगने', 'angane', '3']\n",
            "['अंगभंग', 'angbhang', '3']\n",
            "['अंगरक्षक', 'angarakshak', '1']\n",
            "['अंगरक्षक', 'angrakshak', '2']\n",
            "['अंगारा', 'angara', '3']\n",
            "['अंगारे', 'angaare', '1']\n",
            "['अंगारे', 'angare', '2']\n",
            "['अंगी', 'angi', '3']\n",
            "['अंगीकार', 'angikar', '3']\n",
            "['अंगुठे', 'anguthe', '3']\n",
            "['अंगुल', 'angul', '3']\n",
            "['अंगुलियों', 'anguliyon', '3']\n",
            "['अंगुली', 'anguli', '2']\n",
            "['अंगुली', 'ungli', '1']\n",
            "['अंगूठा', 'angutha', '3']\n",
            "['अंगूठियों', 'aanguthiyon', '1']\n",
            "['अंगूठियों', 'anguthiyon', '2']\n",
            "['अंगूठी', 'anguthi', '3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ap4sT1mhcnJ"
      },
      "source": [
        "  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path_train = \"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "data_path_dev = '/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv'"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoMs0KpchYxh"
      },
      "source": [
        "def load_inputs(data_path,data):\n",
        "  # Vectorize the data.\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  input_characters = set()\n",
        "  target_characters = set()\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      lines = f.read().split(\"\\n\")\n",
        "  for line in lines[: (len(lines) - 1)]:\n",
        "      input_text, target_text, _ = line.split(\"\\t\")\n",
        "      # We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "      for char in input_text:\n",
        "          if char not in input_characters:\n",
        "              input_characters.add(char)\n",
        "      for char in target_text:\n",
        "          if char not in target_characters:\n",
        "              target_characters.add(char)\n",
        "\n",
        "  input_characters = sorted(list(input_characters))\n",
        "  target_characters = sorted(list(target_characters))\n",
        "  num_encoder_tokens = len(input_characters)\n",
        "  num_decoder_tokens = len(target_characters)\n",
        "  global max_encoder_seq_length, max_decoder_seq_length\n",
        "  if data == 'train':\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "  # print(\"Number of samples:\", len(input_texts))\n",
        "  # print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "  # print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "  # print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "  # print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "  target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "  if data == 'train':\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "  if data == 'dev':\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, input_token_train), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, output_token_train), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, output_token_train), dtype=\"float32\"\n",
        "    )\n",
        "  \n",
        "  random.seed(0)\n",
        "  temp = list(zip(input_texts, target_texts))\n",
        "  random.shuffle(temp)\n",
        "  it, tt = zip(*temp)\n",
        "  \n",
        "  for i, (input_text, target_text) in enumerate(zip(it, tt)):\n",
        "      for t, char in enumerate(input_text):\n",
        "          encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "      # encoder_input_data[i, (t + 1):, input_token_index[\" \"]] = 1.0\n",
        "      for t, char in enumerate(target_text):\n",
        "          # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "          decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "          if t > 0:\n",
        "              # decoder_target_data will be ahead by one timestep\n",
        "              # and will not include the start character.\n",
        "              decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "      # decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "      # decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "  return encoder_input_data, decoder_input_data, decoder_target_data, num_encoder_tokens, num_decoder_tokens, input_token_index, target_token_index"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q83Qb1QcASDh"
      },
      "source": [
        "global enc_input_train, dec_input_train, dec_target_train, input_token_train, output_token_train, input_token_index, target_token_index\n",
        "global enc_input_dev, dec_input_dev, dec_target_dev\n",
        "global max_encoder_seq_length, max_decoder_seq_length\n",
        "enc_input_train, dec_input_train, dec_target_train, input_token_train, output_token_train, input_token_index, target_token_index = load_inputs(data_path_train,'train')\n",
        "enc_input_dev, dec_input_dev, dec_target_dev, _, _, _, _ = load_inputs(data_path_dev,'dev')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHFjZGFxpCb"
      },
      "source": [
        "def encoder(enc_out,in_token_no,enc_block, latent_dim, enc_layers):\n",
        "  for i in range(0,enc_layers):\n",
        "    if enc_block =='LSTM':\n",
        "      enc_out ,h ,c = LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      s = [h,c]\n",
        "    elif enc_block =='GRU':\n",
        "      enc_out ,s = GRU(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "    else:\n",
        "      enc_out ,s = SimpleRNN(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "    \n",
        "  return enc_out,s"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tgqA46f6gtJ"
      },
      "source": [
        "def decoder(X_input_dec, hs_init, latent_dim, out_token_no, dec_block, dec_layers, dense_no,drop_no):\n",
        "  out_dec = X_input_dec\n",
        "  hs = hs_init\n",
        "  for i in range (0,dec_layers):\n",
        "    if dec_block == 'LSTM':\n",
        "      out_dec, h, s = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs)\n",
        "      hs = [h,s]\n",
        "    elif dec_block == 'GRU':\n",
        "      out_dec, hs = GRU(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs)\n",
        "    else:\n",
        "      out_dec, hs = SimpleRNN(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "  \n",
        "  # out_dec = Dense(dense_no/2, activation='tanh')(out_dec)\n",
        "  # out_dec = Dropout(drop_no)(out_dec)\n",
        "  out_dec = TimeDistributed(Dense(out_token_no, activation=\"softmax\"))(out_dec)\n",
        "\n",
        "  return out_dec"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAWVP3PL8kdB"
      },
      "source": [
        "def RNN_model(in_token_no, enc_block, latent_dim, out_token_no, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "  X_input_enc = keras.Input(shape=(None, in_token_no))\n",
        "  X_input_dec = keras.Input(shape=(None, out_token_no))\n",
        "  _,s_init = encoder(X_input_enc,in_token_no,enc_block, latent_dim, enc_layers)\n",
        "  out_dec = decoder(X_input_dec,s_init,latent_dim, out_token_no, dec_block, dec_layers, dense_no, drop_no)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "  return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oquP5kRhmm8H"
      },
      "source": [
        "def enc_inference(model,enc_layers):\n",
        "  X_in_enc = model.input[0]\n",
        "  enc_outputs = X_in_enc\n",
        "  for i in range(0,enc_layers):\n",
        "    enc_outputs, h_enc, s_enc = model.get_layer('encoder'+str(i))(enc_outputs)\n",
        "  X_out_enc = [h_enc, s_enc]\n",
        "  enc_model = keras.Model(inputs = X_in_enc, outputs = X_out_enc)\n",
        "\n",
        "  return enc_model\n",
        "\n",
        "def dec_inference(model, latent_dim, dec_layers):\n",
        "  dec_inputs = model.input[1]\n",
        "  print(tf.shape(dec_inputs))\n",
        "  dec_outputs = dec_inputs\n",
        "  dec_input_h = keras.Input(shape=(latent_dim,))\n",
        "  dec_input_s = keras.Input(shape=(latent_dim,))\n",
        "  hs_inputs = [dec_input_h,dec_input_s]\n",
        "  hs_outputs = hs_inputs\n",
        "  for i in range(0,dec_layers):\n",
        "    dec_outputs, h_dec, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state=hs_outputs)\n",
        "    hs_outputs = [h_dec, s_dec]\n",
        "    print(tf.shape(dec_outputs))\n",
        "    print(tf.shape(hs_outputs))\n",
        "  dec_dense = model.layers[-1]\n",
        "  dec_outputs = dec_dense(dec_outputs)\n",
        "  dec_model = keras.Model(inputs = [dec_inputs] + hs_inputs, outputs = [dec_outputs] + hs_outputs)\n",
        "  \n",
        "  return dec_model\n",
        "\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, output_token_train))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, output_token_train))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nddHQjK_zCJ1"
      },
      "source": [
        "  model = RNN_model(input_token_train,'LSTM',512,output_token_train,'LSTM', 256, 0, 2, 2)\n",
        "  # keras.utils.plot_model(model,show_shapes=True, rankdir='TB')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K0_aZnYxBPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5be3f61b-78a0-48ce-fb8d-749cd399f1a7"
      },
      "source": [
        "model.compile(optimizer=RMSprop(1e-2), loss = 'categorical_crossentropy', metrics='accuracy')\n",
        "model.fit([enc_input_train,dec_input_train], dec_target_train,\n",
        "            batch_size = 128,epochs=20, validation_data=([enc_input_dev,dec_input_dev], dec_target_dev))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "346/346 [==============================] - 20s 45ms/step - loss: 1.1496 - accuracy: 0.0655 - val_loss: 0.9713 - val_accuracy: 0.0900\n",
            "Epoch 2/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.8498 - accuracy: 0.1217 - val_loss: 0.8464 - val_accuracy: 0.1773\n",
            "Epoch 3/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.2847 - accuracy: 0.2878 - val_loss: 0.4716 - val_accuracy: 0.2900\n",
            "Epoch 4/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0586 - accuracy: 0.3548 - val_loss: 0.3618 - val_accuracy: 0.3162\n",
            "Epoch 5/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0238 - accuracy: 0.3661 - val_loss: 0.3445 - val_accuracy: 0.3213\n",
            "Epoch 6/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0140 - accuracy: 0.3694 - val_loss: 0.3300 - val_accuracy: 0.3243\n",
            "Epoch 7/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0094 - accuracy: 0.3702 - val_loss: 0.3449 - val_accuracy: 0.3261\n",
            "Epoch 8/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0087 - accuracy: 0.3700 - val_loss: 0.3366 - val_accuracy: 0.3264\n",
            "Epoch 9/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0064 - accuracy: 0.3706 - val_loss: 0.3385 - val_accuracy: 0.3267\n",
            "Epoch 10/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0052 - accuracy: 0.3710 - val_loss: 0.3507 - val_accuracy: 0.3277\n",
            "Epoch 11/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0049 - accuracy: 0.3712 - val_loss: 0.3736 - val_accuracy: 0.3242\n",
            "Epoch 12/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0043 - accuracy: 0.3715 - val_loss: 0.3458 - val_accuracy: 0.3267\n",
            "Epoch 13/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0037 - accuracy: 0.3714 - val_loss: 0.3493 - val_accuracy: 0.3263\n",
            "Epoch 14/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0035 - accuracy: 0.3716 - val_loss: 0.3708 - val_accuracy: 0.3282\n",
            "Epoch 15/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0035 - accuracy: 0.3719 - val_loss: 0.3252 - val_accuracy: 0.3291\n",
            "Epoch 16/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0035 - accuracy: 0.3711 - val_loss: 0.3191 - val_accuracy: 0.3279\n",
            "Epoch 17/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0029 - accuracy: 0.3725 - val_loss: 0.3484 - val_accuracy: 0.3290\n",
            "Epoch 18/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0027 - accuracy: 0.3724 - val_loss: 0.3450 - val_accuracy: 0.3303\n",
            "Epoch 19/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0028 - accuracy: 0.3720 - val_loss: 0.3665 - val_accuracy: 0.3283\n",
            "Epoch 20/20\n",
            "346/346 [==============================] - 15s 42ms/step - loss: 0.0026 - accuracy: 0.3718 - val_loss: 0.3289 - val_accuracy: 0.3310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa9c86181d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK_4GMoPCqJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8cd6ce4-2994-451a-bc06-2df645de39b6"
      },
      "source": [
        "enc_input_dev[10].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19, 63)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRukORA0wCtP"
      },
      "source": [
        "def train(model,latent_dim,enc_layers, dec_layers):\n",
        "  enc_block = enc_inference(model, enc_layers)\n",
        "  dec_block = dec_inference(model,latent_dim, dec_layers)\n",
        "  keras.utils.plot_model(dec_block,show_shapes=True)\n",
        "  input_seq = tf.expand_dims(enc_input_dev[10],0)\n",
        "  decoded_word = decode_sequence(input_seq, enc_block, dec_block)\n",
        "\n",
        "  return decoded_word"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W-vIMA8GhHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba432d7b-783a-4832-c631-49f41c8c1251"
      },
      "source": [
        "enc_block = enc_inference(model, 2)\n",
        "dec_block = dec_inference(model,512, 2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, None, 28], name='tf.compat.v1.shape/Shape:0', description=\"created by layer 'tf.compat.v1.shape'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, None, 512], name='tf.compat.v1.shape_1/Shape:0', description=\"created by layer 'tf.compat.v1.shape_1'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[2, None, 512], name='tf.compat.v1.shape_2/Shape:0', description=\"created by layer 'tf.compat.v1.shape_2'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, None, 512], name='tf.compat.v1.shape_3/Shape:0', description=\"created by layer 'tf.compat.v1.shape_3'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[2, None, 512], name='tf.compat.v1.shape_4/Shape:0', description=\"created by layer 'tf.compat.v1.shape_4'\")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kffwoQyw4_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f3ae43-20fa-40cf-f32c-bf4ccb8cdad1"
      },
      "source": [
        "print(train(model,512,2,2))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, None, 28], name='tf.compat.v1.shape_20/Shape:0', description=\"created by layer 'tf.compat.v1.shape_20'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, None, 512], name='tf.compat.v1.shape_21/Shape:0', description=\"created by layer 'tf.compat.v1.shape_21'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[2, None, 512], name='tf.compat.v1.shape_22/Shape:0', description=\"created by layer 'tf.compat.v1.shape_22'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[None, None, 512], name='tf.compat.v1.shape_23/Shape:0', description=\"created by layer 'tf.compat.v1.shape_23'\")\n",
            "KerasTensor(type_spec=TensorSpec(shape=(3,), dtype=tf.int32, name=None), inferred_value=[2, None, 512], name='tf.compat.v1.shape_24/Shape:0', description=\"created by layer 'tf.compat.v1.shape_24'\")\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa9b60585f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fa9b6065710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "lag\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBK1pYpQVMeg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUPZBlssPUtq"
      },
      "source": [
        "# model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics='accuracy')\n",
        "model.fit([enc_input_train,dec_input_train], dec_target_train,\n",
        "          batch_size = 64,epochs=40, validation_data=([enc_input_dev,dec_input_dev], dec_target_dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOgRw2Vb7FlA"
      },
      "source": [
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq0DRwp1W7_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508b598b-dd74-4778-a8b8-14ad48a106b2"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }\n",
        "\n",
        "parameters_dict = {\n",
        "    'encoder_block':{\n",
        "        'values': ['LSTM']\n",
        "      },\n",
        "    # 'encoder_layers': {\n",
        "    #     'values': [1,2,3]\n",
        "    #   },\n",
        "    'decoder_block':{\n",
        "      'values' : ['LSTM']\n",
        "      },\n",
        "    # 'decoder_layers': {\n",
        "    #     'values': [1,2,3]\n",
        "    #   },\n",
        "    'latent_dimension': {\n",
        "        'values': [256]\n",
        "      },\n",
        "    'hidden_layer': {\n",
        "        'values': [128]\n",
        "      },\n",
        "    'dropout': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'epochs' : {\n",
        "        'values' : [60]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "pprint.pprint(sweep_config)\n",
        "\n",
        "def training_sweep(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        print(input_token_train)\n",
        "        config = wandb.config\n",
        "        print(output_token_train)\n",
        "        model = RNN_model(input_token_train, config.encoder_block, config.latent_dimension, output_token_train, config.decoder_block,  config.hidden_layer, config.dropout)\n",
        "        print('lol')\n",
        "        model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics='accuracy')\n",
        "        print('lol')\n",
        "        history = model.fit([enc_input_train,dec_input_train], dec_target_train, \n",
        "                            epochs=config.epochs,\n",
        "                            validation_data = ([enc_input_dev,dec_input_dev], dec_target_dev),\n",
        "                            callbacks = [WandbCallback()]\n",
        "                            )\n",
        "        print('lol')\n",
        "        name = str(config.encoder_block) + '_' + str(config.encoder_layers) + '_' + str(config.decoder_block) + '_' + str(config.decoder_layers) + '_' + str(config.hidden_layer) + '_' + str(config.dropout) + '_' + str(config.epochs)\n",
        "        location = '/content/drive/MyDrive/Transliteration/' + name\n",
        "        model.save(location)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'method': 'grid',\n",
            " 'parameters': {'decoder_block': {'values': ['LSTM']},\n",
            "                'dropout': {'values': [0]},\n",
            "                'encoder_block': {'values': ['LSTM']},\n",
            "                'epochs': {'values': [60]},\n",
            "                'hidden_layer': {'values': [128]},\n",
            "                'latent_dimension': {'values': [256]}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1shGb7ZrJqH"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-assg3\")\n",
        "wandb.agent(sweep_id, training_sweep)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}