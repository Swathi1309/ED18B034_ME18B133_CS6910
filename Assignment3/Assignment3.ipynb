{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swathi1309/ED18B034_ME18B133_CS6910/blob/main/Assignment3/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppVny72Q-SPg"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout\n",
        "import tarfile\n",
        "import random"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV6TzDuChLd8"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "wandb.init(project=\"CS6910-assg3\", entity=\"narendv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2LNP-C3PD6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcMrqAD0GVTJ"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1gW8TTyr7x"
      },
      "source": [
        "my_tar = tarfile.open('/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0.tar')\n",
        "my_tar.extractall('/content/drive/MyDrive/Dakshina dataset') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pn0kolbGcbE"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYd7sMK4s8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53677e02-a832-4cdb-eaca-29f91d2ad216"
      },
      "source": [
        "import csv\n",
        "hi_to_eng = open(\"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "read_tsv = csv.reader(hi_to_eng, delimiter=\"\\t\")\n",
        "i=0\n",
        "for row in read_tsv:\n",
        "  i +=1\n",
        "  print(row)\n",
        "  if i==30:\n",
        "    break\n",
        "hi_to_eng.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['अं', 'an', '3']\n",
            "['अंकगणित', 'ankganit', '3']\n",
            "['अंकल', 'uncle', '4']\n",
            "['अंकुर', 'ankur', '4']\n",
            "['अंकुरण', 'ankuran', '3']\n",
            "['अंकुरित', 'ankurit', '3']\n",
            "['अंकुश', 'aankush', '1']\n",
            "['अंकुश', 'ankush', '3']\n",
            "['अंग', 'ang', '2']\n",
            "['अंग', 'anga', '1']\n",
            "['अंगद', 'agandh', '1']\n",
            "['अंगद', 'angad', '2']\n",
            "['अंगने', 'angane', '3']\n",
            "['अंगभंग', 'angbhang', '3']\n",
            "['अंगरक्षक', 'angarakshak', '1']\n",
            "['अंगरक्षक', 'angrakshak', '2']\n",
            "['अंगारा', 'angara', '3']\n",
            "['अंगारे', 'angaare', '1']\n",
            "['अंगारे', 'angare', '2']\n",
            "['अंगी', 'angi', '3']\n",
            "['अंगीकार', 'angikar', '3']\n",
            "['अंगुठे', 'anguthe', '3']\n",
            "['अंगुल', 'angul', '3']\n",
            "['अंगुलियों', 'anguliyon', '3']\n",
            "['अंगुली', 'anguli', '2']\n",
            "['अंगुली', 'ungli', '1']\n",
            "['अंगूठा', 'angutha', '3']\n",
            "['अंगूठियों', 'aanguthiyon', '1']\n",
            "['अंगूठियों', 'anguthiyon', '2']\n",
            "['अंगूठी', 'anguthi', '3']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ap4sT1mhcnJ"
      },
      "source": [
        "  # Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path_train = \"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "data_path_dev = '/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv'"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoMs0KpchYxh"
      },
      "source": [
        "def load_inputs(data_path,data):\n",
        "  # Vectorize the data.\n",
        "  input_texts = []\n",
        "  target_texts = []\n",
        "  input_characters = set()\n",
        "  target_characters = set()\n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      lines = f.read().split(\"\\n\")\n",
        "  for line in lines[: (len(lines) - 1)]:\n",
        "      input_text, target_text, _ = line.split(\"\\t\")\n",
        "      # We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "      target_text = \"\\t\" + target_text + \"\\n\"\n",
        "      input_texts.append(input_text)\n",
        "      target_texts.append(target_text)\n",
        "      for char in input_text:\n",
        "          if char not in input_characters:\n",
        "              input_characters.add(char)\n",
        "      for char in target_text:\n",
        "          if char not in target_characters:\n",
        "              target_characters.add(char)\n",
        "\n",
        "  input_characters = sorted(list(input_characters))\n",
        "  target_characters = sorted(list(target_characters))\n",
        "  num_encoder_tokens = len(input_characters)\n",
        "  num_decoder_tokens = len(target_characters)\n",
        "  global max_encoder_seq_length, max_decoder_seq_length\n",
        "  if data == 'train':\n",
        "    max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "    max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "  # print(\"Number of samples:\", len(input_texts))\n",
        "  # print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "  # print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "  # print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "  # print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "  input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "  target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "  if data == 'train':\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        "    )\n",
        "  if data == 'dev':\n",
        "    encoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_encoder_seq_length, input_token_train), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_input_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, output_token_train), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_target_data = np.zeros(\n",
        "        (len(input_texts), max_decoder_seq_length, output_token_train), dtype=\"float32\"\n",
        "    )\n",
        "  \n",
        "  random.seed(0)\n",
        "  temp = list(zip(input_texts, target_texts))\n",
        "  random.shuffle(temp)\n",
        "  it, tt = zip(*temp)\n",
        "  \n",
        "  for i, (input_text, target_text) in enumerate(zip(it, tt)):\n",
        "      for t, char in enumerate(input_text):\n",
        "          encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "      # encoder_input_data[i, (t + 1):, input_token_index[\" \"]] = 1.0\n",
        "      for t, char in enumerate(target_text):\n",
        "          # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "          decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "          if t > 0:\n",
        "              # decoder_target_data will be ahead by one timestep\n",
        "              # and will not include the start character.\n",
        "              decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "      # decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "      # decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "  return encoder_input_data, decoder_input_data, decoder_target_data, num_encoder_tokens, num_decoder_tokens"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q83Qb1QcASDh"
      },
      "source": [
        "global enc_input_train, dec_input_train, dec_target_train, input_token_train, output_token_train\n",
        "global enc_input_dev, dec_input_dev, dec_target_dev\n",
        "global max_encoder_seq_length, max_decoder_seq_length\n",
        "enc_input_train, dec_input_train, dec_target_train, input_token_train, output_token_train = load_inputs(data_path_train,'train')\n",
        "enc_input_dev, dec_input_dev, dec_target_dev, _, _ = load_inputs(data_path_dev,'dev')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHFjZGFxpCb"
      },
      "source": [
        "def encoder(enc_out,in_token_no,enc_block, latent_dim):\n",
        "  for i in range(0,2):\n",
        "    if enc_block =='LSTM':\n",
        "      enc_out ,h ,c = LSTM(latent_dim, return_state=True, return_sequences=True)(enc_out)\n",
        "      s = [h,c]\n",
        "    elif enc_block =='GRU':\n",
        "      enc_out ,s = GRU(latent_dim, return_state=True, return_sequences=True)(enc_out)\n",
        "    else:\n",
        "      enc_out ,s = SimpleRNN(latent_dim, return_state=True, return_sequences=True)(enc_out)\n",
        "    \n",
        "  return enc_out,s"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tgqA46f6gtJ"
      },
      "source": [
        "def decoder(X_input_dec, hs_init, latent_dim, out_token_no, dec_block,dense_no,drop_no):\n",
        "  out_dec = X_input_dec\n",
        "  hs = hs_init\n",
        "  for i in range (0,2):\n",
        "    if dec_block == 'LSTM':\n",
        "      out_dec, h, s = LSTM(latent_dim, return_sequences=True, return_state=True)(out_dec, initial_state = hs)\n",
        "      hs = [h,s]\n",
        "    elif dec_block == 'GRU':\n",
        "      out_dec, hs = GRU(latent_dim, return_sequences=True, return_state=True)(out_dec, initial_state = hs)\n",
        "    else:\n",
        "      out_dec, hs = SimpleRNN(latent_dim, return_sequences=True, return_state=True)(out_dec, initial_state = hs)\n",
        "  \n",
        "  # out_dec = Dense(dense_no/2, activation='tanh')(out_dec)\n",
        "  # out_dec = Dropout(drop_no)(out_dec)\n",
        "  out_dec = Dense(out_token_no, activation=\"softmax\")(out_dec)\n",
        "\n",
        "  return out_dec"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAWVP3PL8kdB"
      },
      "source": [
        "def RNN_model(in_token_no, enc_block, latent_dim, out_token_no, dec_block,dense_no,drop_no):\n",
        "  X_input_enc = keras.Input(shape=(None, in_token_no))\n",
        "  X_input_dec = keras.Input(shape=(None, out_token_no))\n",
        "  _,s_init = encoder(X_input_enc,in_token_no,enc_block,latent_dim)\n",
        "  out_dec = decoder(X_input_dec,s_init,latent_dim, out_token_no, dec_block, dense_no,drop_no)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec)\n",
        "  return model"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_8eeDMhed_v"
      },
      "source": [
        "lat_dim=512\n",
        "model = RNN_model(input_token_train,'LSTM',lat_dim,output_token_train,'LSTM', 256,0)\n",
        "keras.utils.plot_model(model,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ4qALEm0gsv"
      },
      "source": [
        "from keras.optimizers import RMSprop, Adam, SGD"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNsKhvIwzog2"
      },
      "source": [
        "model = RNN_model(input_token_train,'LSTM',lat_dim,output_token_train,'LSTM', 256,0)\n",
        "model.compile(optimizer=RMSprop(1e-2), loss = 'categorical_crossentropy', metrics='accuracy')\n",
        "model.fit([enc_input_train,dec_input_train], dec_target_train,\n",
        "          batch_size = 128,epochs=40, validation_data=([enc_input_dev,dec_input_dev], dec_target_dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oquP5kRhmm8H"
      },
      "source": [
        "def enc_inference(model):\n",
        "  X_in_enc = model.input[0]\n",
        "  enc_out ,h ,s = model.layers[2].output\n",
        "  X_out_enc = [h,s]\n",
        "  enc_model = keras.Model(inputs = X_in_enc, outputs = X_out_enc)\n",
        "\n",
        "  return enc_model\n",
        "\n",
        "def dec_inference(model):\n",
        "  dec_inputs = model.input[1]\n",
        "  dec_input_h = keras.Input(shape=(latent_dim,))\n",
        "  dec_input_s = keras.Input(shape=(latent_dim,))\n",
        "  hs_inputs = [dec_input_h,dec_input_s]\n",
        "  dec_model = model.layers[3]\n",
        "  dec_outputs, h_dec, s_dec = dec_model(dec_inputs, initial_state=hs_inputs)\n",
        "  hs_outputs = [h_dec, s_dec]\n",
        "  dec_dense = model.layers[4]\n",
        "  dec_outputs = dec_dense(dec_outputs)\n",
        "  dec_model = keras.Model(inputs = [dec_inputs] + hs_inputs, outputs = [decoder_outputs]\n",
        "                              + hs_outputs)\n",
        "  \n",
        "  return dec_model\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUPZBlssPUtq"
      },
      "source": [
        "# model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics='accuracy')\n",
        "model.fit([enc_input_train,dec_input_train], dec_target_train,\n",
        "          batch_size = 64,epochs=40, validation_data=([enc_input_dev,dec_input_dev], dec_target_dev))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOgRw2Vb7FlA"
      },
      "source": [
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq0DRwp1W7_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508b598b-dd74-4778-a8b8-14ad48a106b2"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }\n",
        "\n",
        "parameters_dict = {\n",
        "    'encoder_block':{\n",
        "        'values': ['LSTM']\n",
        "      },\n",
        "    # 'encoder_layers': {\n",
        "    #     'values': [1,2,3]\n",
        "    #   },\n",
        "    'decoder_block':{\n",
        "      'values' : ['LSTM']\n",
        "      },\n",
        "    # 'decoder_layers': {\n",
        "    #     'values': [1,2,3]\n",
        "    #   },\n",
        "    'latent_dimension': {\n",
        "        'values': [256]\n",
        "      },\n",
        "    'hidden_layer': {\n",
        "        'values': [128]\n",
        "      },\n",
        "    'dropout': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'epochs' : {\n",
        "        'values' : [60]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "pprint.pprint(sweep_config)\n",
        "\n",
        "def training_sweep(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        print(input_token_train)\n",
        "        config = wandb.config\n",
        "        print(output_token_train)\n",
        "        model = RNN_model(input_token_train, config.encoder_block, config.latent_dimension, output_token_train, config.decoder_block,  config.hidden_layer, config.dropout)\n",
        "        print('lol')\n",
        "        model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics='accuracy')\n",
        "        print('lol')\n",
        "        history = model.fit([enc_input_train,dec_input_train], dec_target_train, \n",
        "                            epochs=config.epochs,\n",
        "                            validation_data = ([enc_input_dev,dec_input_dev], dec_target_dev),\n",
        "                            callbacks = [WandbCallback()]\n",
        "                            )\n",
        "        print('lol')\n",
        "        # name = str(config.encoder_block) + '_' + str(config.encoder_layers) + '_' + str(config.decoder_block) + '_' + str(config.decoder_layers) + '_' + str(config.hidden_layer) + '_' + str(config.dropout) + '_' + str(config.epochs)\n",
        "        # location = '/content/drive/MyDrive/Transliteration/' + name\n",
        "        # model.save(location)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'method': 'grid',\n",
            " 'parameters': {'decoder_block': {'values': ['LSTM']},\n",
            "                'dropout': {'values': [0]},\n",
            "                'encoder_block': {'values': ['LSTM']},\n",
            "                'epochs': {'values': [60]},\n",
            "                'hidden_layer': {'values': [128]},\n",
            "                'latent_dimension': {'values': [256]}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1shGb7ZrJqH"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-assg3\")\n",
        "wandb.agent(sweep_id, training_sweep)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}