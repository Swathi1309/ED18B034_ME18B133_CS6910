{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swathi1309/ED18B034_ME18B133_CS6910/blob/main/Assignment3/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERdcRt08YczS"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppVny72Q-SPg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e82174-e4a7-4b3f-f6d4-f3182428ea15"
      },
      "source": [
        "!pip install webcolors\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout, TimeDistributed, Lambda, Activation, Reshape,\\\n",
        "Softmax, Multiply, AdditiveAttention, Concatenate, Add, RepeatVector\n",
        "import tarfile\n",
        "import random\n",
        "from keras.optimizers import RMSprop, Adam, SGD\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "from math import log\n",
        "from numpy import array, argmax\n",
        "import sklearn\n",
        "\n",
        "import matplotlib\n",
        "from __future__ import unicode_literals\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "from IPython.display import HTML as html_print\n",
        "from IPython.display import display\n",
        "\n",
        "from webcolors import rgb_to_hex\n",
        "\n",
        "import pprint"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: webcolors in /usr/local/lib/python3.7/dist-packages (1.11.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV6TzDuChLd8"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "wandb.init(project=\"assg3\", entity='cs6910_project')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2LNP-C3PD6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEjUih-v4Ad7"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcMrqAD0GVTJ"
      },
      "source": [
        "File extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1gW8TTyr7x"
      },
      "source": [
        "my_tar = tarfile.open('/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0.tar')\n",
        "my_tar.extractall('/content/drive/MyDrive/Dakshina dataset') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pn0kolbGcbE"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYd7sMK4s8_"
      },
      "source": [
        "import csv\n",
        "hi_to_eng = open(\"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "read_tsv = csv.reader(hi_to_eng, delimiter=\"\\t\")\n",
        "i=0\n",
        "for row in read_tsv:\n",
        "  i +=1\n",
        "  print(row)\n",
        "  if i==30:\n",
        "    break\n",
        "hi_to_eng.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZPMhq4O4F78"
      },
      "source": [
        "# Loading datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DWRiV13NFdC"
      },
      "source": [
        "def load_data(data_path, data):\n",
        "  input_texts=[]\n",
        "  target_texts = []\n",
        "  global tk_input, tk_target, num_input_tokens, num_target_tokens\n",
        "  \n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[: (len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    \n",
        "  if (data ==\"test\"):\n",
        "    global  input_texts_test\n",
        "    input_texts_test = input_texts\n",
        "  \n",
        "  if (data == \"train\"):\n",
        "    tk_input = keras.preprocessing.text.Tokenizer(num_words=None, char_level=True)\n",
        "    tk_input.fit_on_texts(input_texts)\n",
        "    enc_input = tk_input.texts_to_sequences(input_texts)\n",
        "    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, padding='post')\n",
        "\n",
        "    tk_target = keras.preprocessing.text.Tokenizer(num_words=None, char_level=True)\n",
        "    tk_target.fit_on_texts(target_texts)\n",
        "    dec_target = tk_target.texts_to_sequences(target_texts)\n",
        "    dec_target = tf.keras.preprocessing.sequence.pad_sequences(dec_target, padding='post')\n",
        "\n",
        "    dec_input = np.zeros(dec_target.shape)\n",
        "    dec_input[:,:(dec_target.shape[1]-1)] = dec_target[:,1:]\n",
        "  \n",
        "  else:  \n",
        "    enc_input = tk_input.texts_to_sequences(input_texts)\n",
        "    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, padding='post', maxlen = max_input_length)\n",
        "\n",
        "    dec_target = tk_target.texts_to_sequences(target_texts)\n",
        "    dec_target = tf.keras.preprocessing.sequence.pad_sequences(dec_target, padding='post', maxlen = max_target_length)\n",
        "\n",
        "    dec_input = np.zeros(dec_target.shape)\n",
        "    dec_input[:,:(dec_target.shape[1]-1)] = dec_target[:,1:]\n",
        "  \n",
        "  return enc_input, dec_target, dec_input"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xJb4hbNNPO-"
      },
      "source": [
        "data_path_train = \"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "data_path_dev = '/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv'\n",
        "data_path_test = '/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv'\n",
        "\n",
        "global enc_input_train, dec_input_train, dec_target_train, enc_input_dev, dec_input_dev, dec_target_dev\n",
        "global tk_input, tk_target, reverse_input_char_index, reverse_target_char_index\n",
        "global input_token_train, output_token_train, max_input_length, max_target_length\n",
        "global y_true_dev, y_true_train\n",
        "global classes\n",
        "global input_texts_test\n",
        "\n",
        "# Loading datasets\n",
        "enc_input_train, dec_input_train, dec_target_train = load_data(data_path_train,'train')\n",
        "\n",
        "max_input_length = enc_input_train.shape[1]\n",
        "max_target_length = dec_target_train.shape[1]\n",
        "\n",
        "num_input_tokens = len(tk_input.word_index)+1\n",
        "num_target_tokens = len(tk_target.word_index)+1\n",
        "\n",
        "enc_input_dev, dec_input_dev, dec_target_dev = load_data(data_path_dev,'dev')\n",
        "enc_input_test, dec_input_test, dec_target_test = load_data(data_path_test,'test')\n",
        "\n",
        "# One hot encoding of target outputs\n",
        "y_true_dev = np.zeros((dec_target_dev.shape[0],dec_target_dev.shape[1], num_target_tokens))\n",
        "for i in range(0,dec_target_dev.shape[0]):\n",
        "  y_true_dev[i] = to_categorical(dec_target_dev[i], num_classes = num_target_tokens)\n",
        "\n",
        "y_true_train = np.zeros((dec_target_train.shape[0],dec_target_train.shape[1], num_target_tokens))\n",
        "for i in range(0,dec_target_train.shape[0]):\n",
        "  y_true_train[i] = to_categorical(dec_target_train[i], num_classes = num_target_tokens)\n",
        "\n",
        "# Integer to character dictionary\n",
        "reverse_input_char_index = dict((i, char) for char, i in tk_input.word_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in tk_target.word_index.items())\n",
        "\n",
        "# Defining classes\n",
        "classes = [\"0\"]\n",
        "for i in range(1,29):\n",
        "  classes.append(reverse_target_char_index[i])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JFCpjQw8WuD"
      },
      "source": [
        "# Defining functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBsU_wpbCkSH"
      },
      "source": [
        "Training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHFjZGFxpCb"
      },
      "source": [
        "# Encoder\n",
        "def encoder(enc_out,enc_block, latent_dim, enc_layers):\n",
        "  states = []\n",
        "  for i in range(0,enc_layers):\n",
        "    if enc_block =='LSTM':\n",
        "      enc_out ,h ,c = LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      s = [h,c]\n",
        "      states.append(s)\n",
        "    elif enc_block =='GRU':\n",
        "      enc_out ,s = GRU(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      states.append(s)\n",
        "    else:\n",
        "      enc_out ,s = SimpleRNN(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      states.append(s)\n",
        "    \n",
        "  return enc_out,states\n",
        "\n",
        "# Decoder\n",
        "def decoder(X_input_dec, hs_init, latent_dim, dec_block, dec_layers, dense_no, drop_no, attn):\n",
        "  out_dec = X_input_dec\n",
        "  hs = hs_init\n",
        "  for i in range (0,dec_layers):\n",
        "    if i == 0 and attn == True:\n",
        "      hs[i] = None\n",
        "    if dec_block == 'LSTM':\n",
        "      out_dec, h, s = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "    elif dec_block == 'GRU':\n",
        "      out_dec, _ = GRU(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "    else:\n",
        "      out_dec, _ = SimpleRNN(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "  \n",
        "  if dense_no != 0:\n",
        "    out_dec = TimeDistributed(Dense(dense_no, activation=\"relu\", name = 'output'), name = 'dense_hidden')(out_dec)\n",
        "    out_dec = Dropout(drop_no)(out_dec)\n",
        "  out_dec = TimeDistributed(Dense(num_target_tokens, activation=\"softmax\", name = 'output'), name = 'dense')(out_dec)\n",
        "  \n",
        "  return out_dec\n",
        "\n",
        "def decoder_att(X_input_dec, hs_init, latent_dim, dec_block, dec_layers, dense_no, drop_no, attn):\n",
        "  out_dec = X_input_dec\n",
        "  hs = hs_init\n",
        "  for i in range (0,dec_layers):\n",
        "    if dec_block == 'LSTM':\n",
        "      out_dec, h, s = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "    elif dec_block == 'GRU':\n",
        "      out_dec, _ = GRU(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "    else:\n",
        "      out_dec, _ = SimpleRNN(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "  return out_dec\n",
        "\n",
        "# Bahnadau Attention\n",
        "def b_attention(latent_dim):\n",
        "  query_in = keras.Input((max_target_length,latent_dim))\n",
        "  values_in = keras.Input((max_input_length,latent_dim))\n",
        "  # query_mask = tf.ones(tf.shape(query)[:-1], dtype= bool)\n",
        "  # value_mask = mask\n",
        "  # query = Reshape((1,latent_dim), name='query_reshape')(query_in)\n",
        "  score_unit2 = Dense(num_target_tokens, name='score_unitq')(query_in)\n",
        "  score_unit1 = Dense(num_target_tokens, name='score_unitv')(values_in)\n",
        "  context_vector, attention_weights = tf.keras.layers.AdditiveAttention()([score_unit2, score_unit1], return_attention_scores = True)\n",
        "  # score_unit12 = Add()([score_unit1, score_unit2])\n",
        "  # score_unit12 = Activation('tanh')(score_unit12)\n",
        "  # score = Dense(1)(score_unit12)\n",
        "  # attention_weights = Softmax(axis=1)(score)\n",
        "  # context_vector = Multiply()([attention_weights,values_in])\n",
        "  # context_vector = Lambda(lambda x: tf.reduce_sum(x,axis=1))(context_vector)\n",
        "  # context_vector = RepeatVector(max_target_length)(context_vector)\n",
        "\n",
        "  model = Model(inputs = [query_in,values_in], outputs = [context_vector, attention_weights], name='attention')\n",
        "  return model\n",
        "\n",
        "# Final RNN model for training\n",
        "def RNN_model(embedding_no, enc_block, latent_dim, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "  X_input_enc = keras.Input(shape=(max_input_length,))\n",
        "  X_input_dec = keras.Input(shape=(max_target_length,))\n",
        "  X_enc = tf.keras.layers.Embedding(num_input_tokens, embedding_no, name = 'embedding_enc')(X_input_enc)\n",
        "  X_dec = tf.keras.layers.Embedding(num_target_tokens, embedding_no, name = 'embedding_dec')(X_input_dec)\n",
        "  _,s_init = encoder(X_enc,enc_block, latent_dim, enc_layers)\n",
        "  out_dec = decoder(X_dec,s_init, latent_dim, dec_block, dec_layers, dense_no, drop_no, False)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "  return model\n",
        "\n",
        "# Final RNN model with attention for training\n",
        "# def RNN_w_att(embedding_no, enc_block, latent_dim, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "#   X_input_enc = keras.Input(shape=(max_input_length,))\n",
        "#   X_input_dec = keras.Input(shape=(max_target_length,))\n",
        "\n",
        "#   X_enc = tf.keras.layers.Embedding(num_input_tokens, embedding_no, name = 'embedding_enc')(X_input_enc)\n",
        "#   X_dec = tf.keras.layers.Embedding(num_target_tokens, embedding_no, name = 'embedding_dec')(X_input_dec)\n",
        "\n",
        "#   values,s_init = encoder(X_enc,enc_block, latent_dim, enc_layers)\n",
        "#   context_vector, weights = b_attention(latent_dim)([s_init[len(s_init)-1][0], values])\n",
        "#   X_dec = Concatenate(name = 'concatenate')([context_vector,X_dec])\n",
        "\n",
        "#   out_dec = decoder(X_dec,s_init, latent_dim, dec_block, dec_layers, dense_no, drop_no, True)\n",
        "#   model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "\n",
        "#   return model\n",
        "\n",
        "def RNN_w_att(embedding_no, enc_block, latent_dim, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "  X_input_enc = keras.Input(shape=(max_input_length,))\n",
        "  X_input_dec = keras.Input(shape=(max_target_length,))\n",
        "\n",
        "  X_enc = tf.keras.layers.Embedding(num_input_tokens, embedding_no, name = 'embedding_enc')(X_input_enc)\n",
        "  X_dec = tf.keras.layers.Embedding(num_target_tokens, embedding_no, name = 'embedding_dec')(X_input_dec)\n",
        "\n",
        "  out_enc ,s_init = encoder(X_enc,enc_block, latent_dim, enc_layers)\n",
        "  # context_vector, weights = b_attention(latent_dim)([s_init[len(s_init)-1][0], values])\n",
        "  # X_dec = Concatenate(name = 'concatenate')([context_vector,X_dec])\n",
        "\n",
        "  out_dec = decoder_att(X_dec,s_init, latent_dim, dec_block, dec_layers, dense_no, drop_no, True)\n",
        "  context_vector, weights = b_attention(latent_dim)([out_dec, out_enc])\n",
        "  out_dec = Concatenate(name = 'concatenate')([context_vector,out_dec])\n",
        "  dense_layer = Dense(num_target_tokens, activation=\"softmax\", name = 'dense')\n",
        "  out_dec = TimeDistributed(dense_layer, name = 'time_dis')(out_dec)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JuwWrzxDE93"
      },
      "source": [
        "Inference models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oquP5kRhmm8H"
      },
      "source": [
        "# Encoder model for inference\n",
        "def enc_inference(model,enc_layers,rnn_block):\n",
        "  X_enc = model.input[0]\n",
        "  X_in_enc = model.get_layer('embedding_enc')(X_enc)\n",
        "  enc_outputs = X_in_enc\n",
        "  states = []\n",
        "  if rnn_block == 'LSTM':\n",
        "    for i in range(0,enc_layers):\n",
        "      enc_outputs, h_enc, s_enc = model.get_layer('encoder'+str(i))(enc_outputs)\n",
        "      states.append([h_enc, s_enc])\n",
        "  else:\n",
        "    for i in range(0,enc_layers):\n",
        "      enc_outputs, s_enc = model.get_layer('encoder'+str(i))(enc_outputs)\n",
        "      states.append(s_enc)\n",
        "  \n",
        "  enc_model = keras.Model(inputs = X_enc, outputs = [enc_outputs, states])\n",
        "  return enc_model\n",
        "\n",
        "# Decoder model for inference\n",
        "def dec_inference(model, latent_dim, dec_layers, rnn_block, attn, embedding_dim):\n",
        "  dec_inputs = model.input[1]\n",
        "  dec_outputs = model.get_layer('embedding_dec')(dec_inputs)\n",
        "  hs_inputs = []\n",
        "  \n",
        "  if rnn_block == 'LSTM':\n",
        "    for i in range(0,dec_layers):\n",
        "      h = keras.Input(shape=(latent_dim,))\n",
        "      c = keras.Input(shape=(latent_dim,))\n",
        "      hs_inputs.append([h,c])\n",
        "    hs_outputs = []\n",
        "    for i in range(0,dec_layers):\n",
        "      dec_outputs, h_dec, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state = hs_inputs[i])\n",
        "      hs_outputs.append([h_dec,s_dec])\n",
        "\n",
        "  else:\n",
        "    for i in range(0,dec_layers):\n",
        "      c = keras.Input(shape=(latent_dim,))\n",
        "      hs_inputs.append(c)\n",
        "    hs_outputs = []\n",
        "    for i in range(0,dec_layers):\n",
        "      dec_outputs, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state = hs_inputs[i])\n",
        "      hs_outputs.append(s_dec)\n",
        "\n",
        "  dec_dense = model.get_layer('dense')\n",
        "  dec_outputs = dec_dense(dec_outputs)\n",
        "  dec_model = keras.Model(inputs = [dec_inputs, hs_inputs], outputs = [dec_outputs, hs_outputs])  \n",
        "  return dec_model\n",
        "\n",
        "# Decoder model for inference with attention\n",
        "def dec_inference_att(model, latent_dim, dec_layers, rnn_block, attn, embedding_dim):\n",
        "  dec_inputs = model.input[1]\n",
        "  dec_outputs = model.get_layer('embedding_dec')(dec_inputs)\n",
        "  hs_inputs = []\n",
        "  \n",
        "  if rnn_block == 'LSTM':\n",
        "    for i in range(0,dec_layers):\n",
        "      h = keras.Input(shape=(latent_dim,))\n",
        "      c = keras.Input(shape=(latent_dim,))\n",
        "      hs_inputs.append([h,c])\n",
        "    hs_outputs = []\n",
        "    for i in range(0,dec_layers):\n",
        "      dec_outputs, h_dec, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state = hs_inputs[i])\n",
        "      hs_outputs.append([h_dec,s_dec])\n",
        "\n",
        "  else:\n",
        "    for i in range(0,dec_layers):\n",
        "      c = keras.Input(shape=(latent_dim,))\n",
        "      hs_inputs.append(c)\n",
        "    hs_outputs = []\n",
        "    for i in range(0,dec_layers):\n",
        "      dec_outputs, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state = hs_inputs[i])\n",
        "      hs_outputs.append(s_dec)\n",
        "\n",
        "  dec_model = keras.Model(inputs = [dec_inputs, hs_inputs], outputs = [dec_outputs, hs_outputs])  \n",
        "  return dec_model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4In3RXxGIaxT"
      },
      "source": [
        "Decoder functions for model without attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPN10ST5IY15"
      },
      "source": [
        "# Greedy decoder\n",
        "def greedy_decoder(enc_inf, dec_inf, input_seq, rnn_block):\n",
        "  _, states = enc_inf.predict(input_seq)\n",
        "  target_seq = np.zeros((input_seq.shape[0],1))\n",
        "  target_seq[:,0] = tk_target.word_index[\"\\t\"]\n",
        "  pred = [[tk_target.word_index[\"\\t\"]] for _ in range (input_seq.shape[0])]\n",
        "  \n",
        "  if rnn_block == 'LSTM':\n",
        "    for i in range(max_target_length):\n",
        "      output_tokens, state_final = dec_inf.predict([target_seq, states])\n",
        "      x = np.argmax(output_tokens, axis=-1)\n",
        "      target_seq = x\n",
        "      for j in range(input_seq.shape[0]):\n",
        "        pred[j].append(target_seq[j][0])\n",
        "      states = state_final\n",
        "\n",
        "  else:\n",
        "    for i in range(max_target_length):\n",
        "      output_tokens, c = dec_inf.predict([target_seq, states])\n",
        "      x = np.argmax(output_tokens, axis=-1)\n",
        "      target_seq = x\n",
        "      for j in range(input_seq.shape[0]):\n",
        "        pred[j].append(target_seq[j][0])\n",
        "      states = c\n",
        "\n",
        "  return pred\n",
        "\n",
        "# Beam search function\n",
        "def beam_search(enc_inf, dec_inf, input_seq, k):\n",
        "  _, states = enc_inf.predict(input_seq)\n",
        "  target_seq = np.zeros((k,1))\n",
        "  target_seq[:,0] = tk_target.word_index[\"\\t\"]\n",
        "  pred = [[[tk_target.word_index[\"\\t\"]], 0.0] for _ in range (k)]\n",
        "\n",
        "  for i in range(max_target_length):\n",
        "    all_candidates = list()\n",
        "    output_tokens, state_final = dec_inf.predict([target_seq, states])\n",
        "    states = state_final\n",
        "    for j in range(len(pred)):\n",
        "      seq, score = pred[j]\n",
        "      for l in range(output_tokens.shape[-1]):\n",
        "        candidate = [seq + [l], score - log(output_tokens[j][0][l])]\n",
        "        all_candidates.append(candidate)\n",
        "      if i==0:\n",
        "        break\n",
        "    ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "    pred = ordered[:k]\n",
        "    for j in range (k):\n",
        "      target_seq[j] = pred[j][0][i+1]\n",
        "\n",
        "  final_pred = ordered[:1]\n",
        "  return (final_pred[0][0])\n",
        "\n",
        "# Beam search decoder\n",
        "def beam_search_decoder(enc_inf, dec_inf, input_seq, k):\n",
        "  pred = []\n",
        "  for i in range(input_seq.shape[0]):\n",
        "    input = input_seq[i:i+1,:]\n",
        "    input = np.concatenate((input,)*k, axis = 0)\n",
        "    pred.append(beam_search(enc_inf, dec_inf, input, k))\n",
        "    if i%100 == 0:\n",
        "      print((i/input_seq.shape[0])*100)\n",
        "  return pred"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s620ERBtUtRw"
      },
      "source": [
        "Decoder function for model with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7qprgya8ili"
      },
      "source": [
        "# Greedy decoder for model with attention\n",
        "def greedy_decoder_att(enc_inf, dec_inf, att_inf, tim_dis, input_seq):\n",
        "  output_enc, s_init = enc_inf.predict(input_seq)\n",
        "  target_seq = np.zeros((input_seq.shape[0],1))\n",
        "  target_seq[:,0] = tk_target.word_index[\"\\t\"]\n",
        "\n",
        "  pred = [[tk_target.word_index[\"\\t\"]] for _ in range (input_seq.shape[0])]\n",
        "  attention_weights = [[] for _ in range(input_seq.shape[0])]\n",
        "  for i in range(max_target_length-1):\n",
        "    # context_vector, weights = att_inf.predict([s_init[len(s_init)-1][0], values])\n",
        "    # output_tokens = Concatenate()([context_vector, model.get_layer('embedding_dec')(target_seq)])\n",
        "    #print (len(s_init))\n",
        "    output_dec, state_final = dec_inf.predict([target_seq, s_init])\n",
        "    output_dec = np.squeeze(output_dec)\n",
        "    output_dec = tf.keras.layers.RepeatVector(22)(output_dec)\n",
        "    context_vector, weights = att_inf.predict([output_dec, output_enc])\n",
        "    output_dec = Concatenate(name = 'concatenate')([context_vector,output_dec])\n",
        "    output_dec = tim_dis(output_dec)\n",
        "    x = np.argmax(output_dec[:,i,:], axis = -1)\n",
        "    for j in range(input_seq.shape[0]):\n",
        "      attention_weights[j].append(weights[j])\n",
        "      target_seq[j][0] = x[j]\n",
        "      pred[j].append(target_seq[j][0])\n",
        "    s_init = state_final\n",
        "\n",
        "  return pred, attention_weights"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-MIXTexI0QQ"
      },
      "source": [
        "Word accuracy metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHGzLag7yzoc"
      },
      "source": [
        "def word_accuracy(true_words, pred_words):\n",
        "  count = len([i for i, j in zip(true_words, pred_words) if i==j])\n",
        "  return count/len(pred_words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ1YkgLoDopr"
      },
      "source": [
        "Encoding to Word conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2zgV0O2iPTo"
      },
      "source": [
        "def transliteration(inputs):\n",
        "  start = tk_target.word_index[\"\\t\"]\n",
        "  end = 0\n",
        "  words = []\n",
        "  for i in range(len(inputs)):\n",
        "    start_index = inputs[i].index(start)\n",
        "    try:\n",
        "      end_index = inputs[i].index(end)\n",
        "      temp = inputs[i][start_index+1:end_index]\n",
        "    except ValueError:\n",
        "      temp = inputs[i][start_index+1:]\n",
        "    word = list(map(reverse_target_char_index.__getitem__, temp))\n",
        "    if (word[-1]==\"\\n\"):\n",
        "      word.pop(-1)\n",
        "    separator = ''\n",
        "    word_new = separator.join(word)\n",
        "    words.append(word_new)\n",
        "  return words"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpWZK4G8dvA"
      },
      "source": [
        "# Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq0DRwp1W7_f"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }\n",
        "\n",
        "parameters_dict = {\n",
        "    'embedding_no':{\n",
        "        'values': [512]\n",
        "      },\n",
        "    'rnn_block':{\n",
        "        'values': ['LSTM']\n",
        "      },\n",
        "    'no_of_layers': {\n",
        "        'values': [2]\n",
        "      },\n",
        "    'latent_dimension': {\n",
        "        'values': [512]\n",
        "      },\n",
        "    'hidden_layer': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'dropout': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'beam_search': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'epochs' : {\n",
        "        'values' : [20]\n",
        "      },\n",
        "    'optimizer' : {\n",
        "        'values' : ['rmsprop']\n",
        "      },\n",
        "    'learning_rate' : {\n",
        "        'values' : [1e-3]\n",
        "      },\n",
        "    'attention' : {\n",
        "        'values' : [True]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "pprint.pprint(sweep_config)\n",
        "\n",
        "def training_sweep(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "        if config.attention == True:\n",
        "          model = RNN_w_att(config.embedding_no, config.rnn_block, config.latent_dimension, config.rnn_block, config.hidden_layer, config.dropout, config.no_of_layers, config.no_of_layers)\n",
        "        else:\n",
        "          model = RNN_model(config.embedding_no, config.rnn_block, config.latent_dimension, config.rnn_block,config.hidden_layer, config.dropout, config.no_of_layers, config.no_of_layers)\n",
        "        \n",
        "        if config.optimizer == 'adam':\n",
        "          opt = Adam(config.learning_rate)\n",
        "        else:\n",
        "          opt = RMSprop(config.learning_rate)\n",
        "\n",
        "        model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
        "        model.fit([enc_input_train,dec_input_train], y_true_train, \n",
        "                            epochs=config.epochs,\n",
        "                            validation_data = ([enc_input_dev,dec_input_dev], y_true_dev),\n",
        "                            callbacks = [WandbCallback()]\n",
        "                            )\n",
        "\n",
        "        name = str(config.rnn_block) + '_' + str(config.no_of_layers) + '_' + str(config.embedding_no) + '_' + str(config.latent_dimension) + '_' + str(config.attention) + '_' + str(config.beam_search)\n",
        "        location = '/content/drive/MyDrive/Transliteration/' + name\n",
        "        model.save(location)\n",
        "\n",
        "        if config.attention == False:\n",
        "          enc_inf = enc_inference(model, config.no_of_layers, config.rnn_block)\n",
        "          dec_inf = dec_inference(model, config.latent_dimension, config.no_of_layers, config.rnn_block, config.attention, config.embedding_no)\n",
        "          if (config.beam_search!=0):\n",
        "            predictions = beam_search_decoder(enc_inf, dec_inf, enc_input_dev, config.beam_search)\n",
        "          else:\n",
        "            predictions = greedy_decoder(enc_inf, dec_inf, enc_input_dev, config.rnn_block)\n",
        "        \n",
        "        else:\n",
        "          enc_inf = enc_inference(model, config.no_of_layers, config.rnn_block)\n",
        "          dec_inf = dec_inference_att(model, config.latent_dimension, config.no_of_layers, config.rnn_block, True, config.embedding_no)\n",
        "          att_inf = model.get_layer('attention')\n",
        "          tim_dis = model.get_layer('time_dis')\n",
        "          predictions, weights = greedy_decoder_att(enc_inf, dec_inf, att_inf, tim_dis, enc_input_dev)\n",
        "\n",
        "        pred_words = transliteration(predictions)\n",
        "\n",
        "        true = list(list(dec_input_dev[i]) for i in range(dec_input_dev.shape[0]))\n",
        "        true_words = transliteration(list(true))\n",
        "        word_acc = word_accuracy(true_words, pred_words)\n",
        "        wandb.log({'word_level_accuracy': word_acc})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1shGb7ZrJqH"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"assg3\")\n",
        "wandb.agent(sweep_id, training_sweep)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rD4khfCeDg64"
      },
      "source": [
        "# Best model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIMSK26sDwo_"
      },
      "source": [
        "# Function to save predictions\n",
        "def save_predictions(true_test, pred_test, attn):\n",
        "  global input_texts_test\n",
        "  if attn==True:\n",
        "    f = open('/content/drive/MyDrive/Transliteration/predictions_attention', \"w\")\n",
        "  else:\n",
        "    f = open('/content/drive/MyDrive/Transliteration/predictions_vanilla', \"w\")\n",
        "  for i in range(len(input_texts_test)):\n",
        "    line = input_texts_test[i]+\"\\t\"+true_test[i]+\"\\t\"+pred_test[i]+\"\\n\"\n",
        "    f.write(line)\n",
        "  f.close()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NarUpfA4Mtb1"
      },
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/Transliteration/LSTM_2_512_512_0_20')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N42N6jYKEAaY"
      },
      "source": [
        "enc_inf = enc_inference(model, 2, \"LSTM\")\n",
        "dec_inf = dec_inference(model, 512, 2, 'LSTM', False, 512)\n",
        "\n",
        "predictions = greedy_decoder(enc_inf, dec_inf, enc_input_test, 'LSTM')\n",
        "pred_words = transliteration(predictions)\n",
        "\n",
        "true = list(list(dec_input_test[i]) for i in range(dec_input_test.shape[0]))\n",
        "true_words = transliteration(list(true))\n",
        "\n",
        "word_acc = word_accuracy(true_words, pred_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjhMuVx6E5b2"
      },
      "source": [
        "print (word_acc)\n",
        "save_predictions(true_words, pred_words, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39j_dgjQGZ-g"
      },
      "source": [
        "# Plotting confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB9UTuIDpx32"
      },
      "source": [
        "def labels(true_words, pred_words):\n",
        "  true = []\n",
        "  pred = []\n",
        "  for i in range(len(true_words)):\n",
        "    word1 = true_words[i]\n",
        "    word2 = pred_words[i]\n",
        "    len1 = len(word1)\n",
        "    len2 = len(word2)\n",
        "    \n",
        "    for letter1 in word1:\n",
        "      true.append(tk_target.word_index[letter1])\n",
        "    for letter2 in word2:\n",
        "      pred.append(tk_target.word_index[letter2])\n",
        "    \n",
        "    if len1<len2:\n",
        "      for i in range(len2-len1):\n",
        "        true.append(0)\n",
        "    if len1>len2:\n",
        "      for i in range(len1-len2):\n",
        "        pred.append(0)\n",
        "\n",
        "  return true, pred\n",
        "\n",
        "def confusion_matrix(true, pred):\n",
        "  wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None, y_true=true, preds=pred, class_names=classes)})"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrQtJipmFVXl"
      },
      "source": [
        "true, pred = labels(true_words, pred_words)\n",
        "confusion_matrix(true,pred)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0waSmjAdbk6"
      },
      "source": [
        "# Best model with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBhSjW_Bdfqg"
      },
      "source": [
        "model_att = keras.models.load_model('/content/drive/MyDrive/Transliteration/LSTM_2_512_512_True_0')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWWcg_XyeB46"
      },
      "source": [
        "enc_inf = enc_inference(model_att, 2, 'LSTM')\n",
        "dec_inf = dec_inference_att(model_att, 512, 2, 'LSTM', True, 512)\n",
        "att_inf = model_att.get_layer('attention')\n",
        "tim_dis = model_att.get_layer('time_dis')\n",
        "\n",
        "predictions, weights = greedy_decoder_att(enc_inf, dec_inf, att_inf, tim_dis, enc_input_test)\n",
        "pred_words = transliteration(predictions)\n",
        "\n",
        "true = list(list(dec_input_test[i]) for i in range(dec_input_test.shape[0]))\n",
        "true_words = transliteration(list(true))\n",
        "\n",
        "word_acc = word_accuracy(true_words, pred_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTSQYobAfAVO"
      },
      "source": [
        "print (word_acc)\n",
        "save_predictions(true_words, pred_words, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aZufzizU0fh"
      },
      "source": [
        "# Plotting attention heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZS00qHhglhB"
      },
      "source": [
        "def get_attention_weights(weights, num):\n",
        "  weight = []\n",
        "  this_sample = weights[num]\n",
        "  for i in range(21):\n",
        "    weight.append(this_sample[i][0])\n",
        "  weight = np.squeeze(np.array(weight))\n",
        "  return weight"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E7az615UzsJ"
      },
      "source": [
        "def plot_attention(attention, input_word, predicted_word):\n",
        "  attention = attention[:len(pred), :len(word)]\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='gray')\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  # ax.set_xticklabels([''] + list(input_word), rotation=90)\n",
        "  ax.set_yticklabels([''] + list(predicted_word))\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  plt.show()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7YPpQVJVCZT"
      },
      "source": [
        "num = np.random.randint(0, len(input_texts_test))\n",
        "word = input_texts_test[num]\n",
        "pred = pred_words[num]\n",
        "weight = get_attention_weights(weights, num)\n",
        "print (list(word))\n",
        "plot_attention(weight, word, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5m1ILIGgeKZ"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH1ZWDTIS2G8"
      },
      "source": [
        "def cstr(s, color='black', indent = False):\n",
        "\tif s == '':\n",
        "\t\treturn \"<text style=color:#000;font-family:verdana;padding-left:50px;background-color:{}> </text>\".format(color, s)   #position:relative;\n",
        "\telse:\n",
        "\t\treturn \"<text style=color:#000;font-family:verdana;background-color:{}>{} </text>\".format(color, s)\n",
        "\n",
        "def print_color(t):\n",
        "\tdisplay(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
        " \n",
        "def grad_color(value):\n",
        "  gb = int(value*510)\n",
        "  hex_color = rgb_to_hex((255,255-gb,255-gb))\n",
        "  return hex_color\n",
        "\n",
        "def final_vis(attention_weights, input_word, output_word):\n",
        "  input_word_list = list(input_word)\n",
        "  output_word_list = list(output_word)\n",
        "  # Let output_values be of shape (num_tokens_output_word,num_tokens_input_word)\n",
        "  # Print the input with the colour fills\n",
        "  for i in range(len(output_word_list)):  # For each word\n",
        "    text_colours = []\n",
        "    for j in range(len(input_word_list)):\n",
        "      text = (input_word_list[j], grad_color(attention_weights[i][j]))\n",
        "      text_colours.append(text)\n",
        "    print_color(text_colours)\n",
        "    print(''.join(output_word_list[0:i]) + \"\\033[4m\" + output_word_list[i] + \"\\033[0m\" + ''.join(output_word_list[i+1:]))\n",
        "    print('  ')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnGBsf-YghlC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "07e94477-ec90-4c3e-c093-78e3f45e43e5"
      },
      "source": [
        "num = np.random.randint(0, len(input_texts_test))\n",
        "word = input_texts_test[num]\n",
        "pred = pred_words[num]\n",
        "weight = get_attention_weights(weights, num)\n",
        "final_vis(weight, word, pred)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ff0000>व </text><text style=color:#000;font-family:verdana;background-color:#fffefe>ा </text><text style=color:#000;font-family:verdana;background-color:#fffdfd>ं </text><text style=color:#000;font-family:verdana;background-color:#fffdfd>ग </text><text style=color:#000;font-family:verdana;background-color:#fffefe>च </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ु </text><text style=color:#000;font-family:verdana;background-color:#ffffff>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[4mv\u001b[0mangachuk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ff9898>व </text><text style=color:#000;font-family:verdana;background-color:#ff3b3b>ा </text><text style=color:#000;font-family:verdana;background-color:#ff4747>ं </text><text style=color:#000;font-family:verdana;background-color:#fff9f9>ग </text><text style=color:#000;font-family:verdana;background-color:#ffeded>च </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ु </text><text style=color:#000;font-family:verdana;background-color:#ffffff>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "v\u001b[4ma\u001b[0mngachuk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#fffafa>व </text><text style=color:#000;font-family:verdana;background-color:#fffdfd>ा </text><text style=color:#000;font-family:verdana;background-color:#ff0000>ं </text><text style=color:#000;font-family:verdana;background-color:#fffcfc>ग </text><text style=color:#000;font-family:verdana;background-color:#ffffff>च </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ु </text><text style=color:#000;font-family:verdana;background-color:#ffffff>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "va\u001b[4mn\u001b[0mgachuk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ffffff>व </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ा </text><text style=color:#000;font-family:verdana;background-color:#fff9f9>ं </text><text style=color:#000;font-family:verdana;background-color:#ff0000>ग </text><text style=color:#000;font-family:verdana;background-color:#fffbfb>च </text><text style=color:#000;font-family:verdana;background-color:#fffefe>ु </text><text style=color:#000;font-family:verdana;background-color:#ffffff>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "van\u001b[4mg\u001b[0machuk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ffffff>व </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ा </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ं </text><text style=color:#000;font-family:verdana;background-color:#fffdfd>ग </text><text style=color:#000;font-family:verdana;background-color:#ff0000>च </text><text style=color:#000;font-family:verdana;background-color:#ff7a7a>ु </text><text style=color:#000;font-family:verdana;background-color:#fff9f9>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "vang\u001b[4ma\u001b[0mchuk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ffffff>व </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ा </text><text style=color:#000;font-family:verdana;background-color:#fffdfd>ं </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ग </text><text style=color:#000;font-family:verdana;background-color:#ff0000>च </text><text style=color:#000;font-family:verdana;background-color:#fff8f8>ु </text><text style=color:#000;font-family:verdana;background-color:#fff0f0>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "vanga\u001b[4mc\u001b[0mhuk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ffffff>व </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ा </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ं </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ग </text><text style=color:#000;font-family:verdana;background-color:#fff6f6>च </text><text style=color:#000;font-family:verdana;background-color:#ff0000>ु </text><text style=color:#000;font-family:verdana;background-color:#ffc8c8>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "vangac\u001b[4mh\u001b[0muk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ffffff>व </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ा </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ं </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ग </text><text style=color:#000;font-family:verdana;background-color:#ffcfcf>च </text><text style=color:#000;font-family:verdana;background-color:#ff4646>ु </text><text style=color:#000;font-family:verdana;background-color:#ff7070>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "vangach\u001b[4mu\u001b[0mk\n",
            "  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<text style=color:#000;font-family:verdana;background-color:#ffffff>व </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ा </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ं </text><text style=color:#000;font-family:verdana;background-color:#ffffff>ग </text><text style=color:#000;font-family:verdana;background-color:#ffffff>च </text><text style=color:#000;font-family:verdana;background-color:#fffcfc>ु </text><text style=color:#000;font-family:verdana;background-color:#ff0000>क </text>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "vangachu\u001b[4mk\u001b[0m\n",
            "  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}