{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swathi1309/ED18B034_ME18B133_CS6910/blob/main/Assignment3/Assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERdcRt08YczS"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppVny72Q-SPg"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model\n",
        "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout, TimeDistributed, Lambda, Activation, Reshape,\\\n",
        "Softmax, Multiply, AdditiveAttention, Concatenate, Add, RepeatVector\n",
        "import tarfile\n",
        "import random\n",
        "from keras.optimizers import RMSprop, Adam, SGD\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV6TzDuChLd8"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "wandb.init(project=\"CS6910-assg3\", entity=\"narendv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d2LNP-C3PD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd542616-4b8f-42e8-c1b4-b6fd20377f77"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEjUih-v4Ad7"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcMrqAD0GVTJ"
      },
      "source": [
        "Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV1gW8TTyr7x"
      },
      "source": [
        "my_tar = tarfile.open('/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0.tar')\n",
        "my_tar.extractall('/content/drive/MyDrive/Dakshina dataset') # specify which folder to extract to\n",
        "my_tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pn0kolbGcbE"
      },
      "source": [
        "Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYd7sMK4s8_"
      },
      "source": [
        "import csv\n",
        "hi_to_eng = open(\"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n",
        "read_tsv = csv.reader(hi_to_eng, delimiter=\"\\t\")\n",
        "i=0\n",
        "for row in read_tsv:\n",
        "  i +=1\n",
        "  print(row)\n",
        "  if i==30:\n",
        "    break\n",
        "hi_to_eng.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZPMhq4O4F78"
      },
      "source": [
        "# Loading datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DWRiV13NFdC"
      },
      "source": [
        "def load_data(data_path, data):\n",
        "  input_texts=[]\n",
        "  target_texts = []\n",
        "  global tk_input, tk_target, num_input_tokens, num_target_tokens\n",
        "  \n",
        "  with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "  for line in lines[: (len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "     \n",
        "  if (data == \"train\"):\n",
        "    tk_input = keras.preprocessing.text.Tokenizer(num_words=None, char_level=True)\n",
        "    tk_input.fit_on_texts(input_texts)\n",
        "    enc_input = tk_input.texts_to_sequences(input_texts)\n",
        "    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, padding='post')\n",
        "\n",
        "    tk_target = keras.preprocessing.text.Tokenizer(num_words=None, char_level=True)\n",
        "    tk_target.fit_on_texts(target_texts)\n",
        "    dec_target = tk_target.texts_to_sequences(target_texts)\n",
        "    dec_target = tf.keras.preprocessing.sequence.pad_sequences(dec_target, padding='post')\n",
        "\n",
        "    dec_input = np.zeros(dec_target.shape)\n",
        "    dec_input[:,:(dec_target.shape[1]-1)] = dec_target[:,1:]\n",
        "  \n",
        "  else:\n",
        "    enc_input = tk_input.texts_to_sequences(input_texts)\n",
        "    enc_input = tf.keras.preprocessing.sequence.pad_sequences(enc_input, padding='post', maxlen = max_input_length)\n",
        "\n",
        "    dec_target = tk_target.texts_to_sequences(target_texts)\n",
        "    dec_target = tf.keras.preprocessing.sequence.pad_sequences(dec_target, padding='post', maxlen = max_target_length)\n",
        "\n",
        "    dec_input = np.zeros(dec_target.shape)\n",
        "    dec_input[:,:(dec_target.shape[1]-1)] = dec_target[:,1:]\n",
        "  \n",
        "  return enc_input, dec_target, dec_input"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ap4sT1mhcnJ"
      },
      "source": [
        "# Number of samples to train on.\n",
        "# Path to the data txt file on disk.\n",
        "data_path_train = \"/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
        "data_path_dev = '/content/drive/MyDrive/Dakshina dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xJb4hbNNPO-"
      },
      "source": [
        "global enc_input_train, dec_input_train, dec_target_train, enc_input_dev, dec_input_dev, dec_target_dev\n",
        "global tk_input, tk_target\n",
        "global input_token_train, output_token_train, max_input_length, max_target_length\n",
        "global y_true_dev, y_true_train\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "enc_input_train, dec_input_train, dec_target_train = load_data(data_path_train,'train')\n",
        "\n",
        "max_input_length = enc_input_train.shape[1]\n",
        "max_target_length = dec_target_train.shape[1]\n",
        "\n",
        "num_input_tokens = len(tk_input.word_index)+1\n",
        "num_target_tokens = len(tk_target.word_index)+1\n",
        "\n",
        "enc_input_dev, dec_input_dev, dec_target_dev = load_data(data_path_dev,'dev')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e06zEFkwZGnZ"
      },
      "source": [
        "y_true_dev = np.zeros((dec_target_dev.shape[0],dec_target_dev.shape[1], num_target_tokens))\n",
        "for i in range(0,dec_target_dev.shape[0]):\n",
        "  y_true_dev[i] = to_categorical(dec_target_dev[i], num_classes = num_target_tokens)\n",
        "\n",
        "y_true_train = np.zeros((dec_target_train.shape[0],dec_target_train.shape[1], num_target_tokens))\n",
        "for i in range(0,dec_target_train.shape[0]):\n",
        "  y_true_train[i] = to_categorical(dec_target_train[i], num_classes = num_target_tokens)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JFCpjQw8WuD"
      },
      "source": [
        "# Defining functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJHFjZGFxpCb"
      },
      "source": [
        "def encoder(enc_out,enc_block, latent_dim, enc_layers):\n",
        "  states = []\n",
        "  for i in range(0,enc_layers):\n",
        "    if enc_block =='LSTM':\n",
        "      enc_out ,h ,c = LSTM(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      s = [h,c]\n",
        "      states.append(s)\n",
        "    elif enc_block =='GRU':\n",
        "      enc_out ,s = GRU(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      states.append(s)\n",
        "    else:\n",
        "      enc_out ,s = SimpleRNN(latent_dim, return_state=True, return_sequences=True, name='encoder'+str(i))(enc_out)\n",
        "      states.append(s)\n",
        "    \n",
        "  return enc_out,states"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1BDb-AY_D9I"
      },
      "source": [
        "def b_attention(latent_dim):\n",
        "\n",
        "  query_in = keras.Input((latent_dim,))\n",
        "  values_in = keras.Input((max_input_length,latent_dim))\n",
        "  query = Reshape((1,latent_dim), name='query_reshape')(query_in)\n",
        "  score_unit2 = Dense(num_target_tokens, name='score_unitq')(query)\n",
        "  score_unit1 = Dense(num_target_tokens, name='score_unitv')(values_in)\n",
        "  score_unit12 = Add()([score_unit1, score_unit2])\n",
        "  score_unit12 = Activation('tanh')(score_unit12)\n",
        "  score = Dense(1)(score_unit12)\n",
        "  attention_weights = Softmax(axis=1)(score)\n",
        "  context_vector = Multiply()([attention_weights,values_in])\n",
        "  context_vector = Lambda(lambda x: tf.reduce_sum(x,axis=1))(context_vector)\n",
        "  context_vector = RepeatVector(max_target_length)(context_vector)\n",
        "\n",
        "  model = Model(inputs = [query_in,values_in], outputs = [context_vector, attention_weights], name='attention')\n",
        "  return model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tgqA46f6gtJ"
      },
      "source": [
        "def decoder(X_input_dec, hs_init, latent_dim, dec_block, dec_layers, dense_no, drop_no):\n",
        "  out_dec = X_input_dec\n",
        "  hs = hs_init\n",
        "  for i in range (0,dec_layers):\n",
        "    if dec_block == 'LSTM':\n",
        "      out_dec, h, s = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "      # hs = [h,s]\n",
        "    elif dec_block == 'GRU':\n",
        "      out_dec, _ = GRU(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "    else:\n",
        "      out_dec, _ = SimpleRNN(latent_dim, return_sequences=True, return_state=True, name='decoder'+str(i))(out_dec, initial_state = hs[i])\n",
        "\n",
        "  out_dec = TimeDistributed(Dense(num_target_tokens, activation=\"softmax\", name = 'output'), name = 'dense')(out_dec)\n",
        "  return out_dec"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WpWZK4G8dvA"
      },
      "source": [
        "# Rest of the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAWVP3PL8kdB"
      },
      "source": [
        "def RNN_model(embedding_no, enc_block, latent_dim, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "  X_input_enc = keras.Input(shape=(max_input_length,))\n",
        "  X_input_dec = keras.Input(shape=(max_target_length,))\n",
        "  X_enc = tf.keras.layers.Embedding(num_input_tokens, embedding_no, name = 'embedding_enc')(X_input_enc)\n",
        "  X_dec = tf.keras.layers.Embedding(num_target_tokens, embedding_no, name = 'embedding_dec')(X_input_dec)\n",
        "  _,s_init = encoder(X_enc,enc_block, latent_dim, enc_layers)\n",
        "  out_dec = decoder(X_dec,s_init, latent_dim, dec_block, dec_layers, dense_no, drop_no)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6s5BUk5Q9dU"
      },
      "source": [
        "def RNN_w_att(embedding_no, enc_block, latent_dim, dec_block, dense_no, drop_no, enc_layers, dec_layers):\n",
        "  X_input_enc = keras.Input(shape=(max_input_length,))\n",
        "  X_input_dec = keras.Input(shape=(max_target_length,))\n",
        "\n",
        "  X_enc = tf.keras.layers.Embedding(num_input_tokens, embedding_no, name = 'embedding_enc')(X_input_enc)\n",
        "  X_dec = tf.keras.layers.Embedding(num_target_tokens, embedding_no, name = 'embedding_dec')(X_input_dec)\n",
        "\n",
        "  values,s_init = encoder(X_enc,enc_block, latent_dim, enc_layers)\n",
        "  context_vector, weights = b_attention(latent_dim)([s_init[len(s_init)-1][0], values])\n",
        "  X_dec = Concatenate(name = 'concatenate')([context_vector,X_dec])\n",
        "\n",
        "  out_dec = decoder(X_dec,s_init, latent_dim, dec_block, dec_layers, dense_no, drop_no)\n",
        "  model = Model(inputs = [X_input_enc,X_input_dec], outputs = out_dec, name = 'train_model')\n",
        "\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nddHQjK_zCJ1"
      },
      "source": [
        "model = RNN_model(256, 'LSTM', 512, 'LSTM', 256, 0, 2, 2)\n",
        "keras.utils.plot_model(model,show_shapes=True, rankdir='TB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K0_aZnYxBPA"
      },
      "source": [
        "model.compile(optimizer=Adam(1e-2), loss = 'categorical_crossentropy', metrics=['accuracy'])#, word_accuracy])\n",
        "model.fit([enc_input_train,dec_input_train], y_true_train, \n",
        "          batch_size = 128,epochs=10, validation_data=([enc_input_dev,dec_input_dev], y_true_dev))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I0A5_Q7abEu"
      },
      "source": [
        "def word_accuracy(y_true,y_pred):\n",
        "  acc = tf.cast(tf.math.reduce_all(tf.math.reduce_all(tf.equal(y_pred,y_true),-1),-1),dtype=tf.int32)\n",
        "  return tf.reduce_sum(acc)/tf.shape(y_true)[0]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oquP5kRhmm8H"
      },
      "source": [
        "def enc_inference(model,enc_layers):\n",
        "  X_enc = model.input[0]\n",
        "  X_in_enc = model.get_layer('embedding_enc')(X_enc)\n",
        "  enc_outputs = X_in_enc\n",
        "  states = []\n",
        "  for i in range(0,enc_layers):\n",
        "    enc_outputs, h_enc, s_enc = model.get_layer('encoder'+str(i))(enc_outputs)\n",
        "    states.append([h_enc, s_enc])\n",
        "  enc_model = keras.Model(inputs = X_enc, outputs = states)\n",
        "  return enc_model\n",
        "\n",
        "def dec_inference(model, latent_dim, dec_layers):\n",
        "  dec_inputs = model.input[1]\n",
        "  dec_outputs = model.get_layer('embedding_dec')(dec_inputs)\n",
        "  hs_inputs = []\n",
        "  for i in range(0,dec_layers):\n",
        "    h = keras.Input(shape=(latent_dim,))\n",
        "    c = keras.Input(shape=(latent_dim,))\n",
        "    hs_inputs.append([h,c])\n",
        "  hs_outputs = []\n",
        "  for i in range(0,dec_layers):\n",
        "    dec_outputs, h_dec, s_dec = model.get_layer('decoder'+str(i))(dec_outputs, initial_state = hs_inputs[i])\n",
        "    hs_outputs.append([h_dec,s_dec])\n",
        "  dec_dense = model.get_layer('dense')\n",
        "  dec_outputs = dec_dense(dec_outputs)\n",
        "  dec_model = keras.Model(inputs = [dec_inputs] + hs_inputs, outputs = [dec_outputs] + hs_outputs)  \n",
        "  return dec_model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXKZ-T7RUqsE"
      },
      "source": [
        "enc_inf = enc_inference(model, 2)\n",
        "dec_inf = dec_inference(model, 512, 2)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00iS3QSsHuwD"
      },
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in tk_input.word_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in tk_target.word_index.items())"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xz4TBCM22Ef"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  states = enc_inf.predict(input_seq)\n",
        "  target_seq = np.zeros((input_seq.shape[0],1))\n",
        "  outputs = [\"\"]*input_seq.shape[0]\n",
        "  target_seq[:,0] = tk_target.word_index[\"\\t\"]\n",
        "  stop_key = tk_target.word_index[\"\\n\"]\n",
        "  outputs = [\"\"]*input_seq.shape[0]\n",
        "  for i in range(input_seq.shape[0]):\n",
        "      outputs[i] = outputs[i]+ str(reverse_target_char_index[target_seq[i][0]])\n",
        "  for i in range(max_target_length):\n",
        "    output_tokens, h, c = dec_inf.predict([target_seq] + states)\n",
        "    x = np.argmax(output_tokens, axis=-1)\n",
        "    target_seq = x\n",
        "    for j in range(input_seq.shape[0]):\n",
        "      outputs[j] = outputs[j]+ str(reverse_target_char_index[target_seq[j][0]])\n",
        "    states = [h,c]\n",
        "  return outputs"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUvRTuMVG2gs"
      },
      "source": [
        "input_seq = enc_input_dev[0:2,:]\n",
        "pred = decode_sequence(input_seq)\n",
        "print (pred)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRukORA0wCtP"
      },
      "source": [
        "def predict(model,latent_dim,enc_layers, dec_layers):\n",
        "  enc_block = enc_inference(model, enc_layers)\n",
        "  dec_block = dec_inference(model,latent_dim, dec_layers)\n",
        "  # keras.utils.plot_model(dec_block,show_shapes=True)\n",
        "  #input_seq = tf.expand_dims(enc_input_dev[10],0)\n",
        "  input_seq = tf.expand_dims(enc_input_dev[0], 0)\n",
        "  decoded_word = decode_sequence(input_seq, enc_block, dec_block)\n",
        "\n",
        "  return decoded_word"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOgRw2Vb7FlA"
      },
      "source": [
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq0DRwp1W7_f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "508b598b-dd74-4778-a8b8-14ad48a106b2"
      },
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }\n",
        "\n",
        "parameters_dict = {\n",
        "    'encoder_block':{\n",
        "        'values': ['LSTM']\n",
        "      },\n",
        "    # 'encoder_layers': {\n",
        "    #     'values': [1,2,3]\n",
        "    #   },\n",
        "    'decoder_block':{\n",
        "      'values' : ['LSTM']\n",
        "      },\n",
        "    # 'decoder_layers': {\n",
        "    #     'values': [1,2,3]\n",
        "    #   },\n",
        "    'latent_dimension': {\n",
        "        'values': [256]\n",
        "      },\n",
        "    'hidden_layer': {\n",
        "        'values': [128]\n",
        "      },\n",
        "    'dropout': {\n",
        "        'values': [0]\n",
        "      },\n",
        "    'epochs' : {\n",
        "        'values' : [60]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "pprint.pprint(sweep_config)\n",
        "\n",
        "def training_sweep(config=None):\n",
        "    with wandb.init(config=config):\n",
        "        print(input_token_train)\n",
        "        config = wandb.config\n",
        "        print(output_token_train)\n",
        "        model = RNN_model(input_token_train, config.encoder_block, config.latent_dimension, output_token_train, config.decoder_block,  config.hidden_layer, config.dropout)\n",
        "        print('lol')\n",
        "        model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', metrics='accuracy')\n",
        "        print('lol')\n",
        "        history = model.fit([enc_input_train,dec_input_train], dec_target_train, \n",
        "                            epochs=config.epochs,\n",
        "                            validation_data = ([enc_input_dev,dec_input_dev], dec_target_dev),\n",
        "                            callbacks = [WandbCallback()]\n",
        "                            )\n",
        "        print('lol')\n",
        "        name = str(config.encoder_block) + '_' + str(config.encoder_layers) + '_' + str(config.decoder_block) + '_' + str(config.decoder_layers) + '_' + str(config.hidden_layer) + '_' + str(config.dropout) + '_' + str(config.epochs)\n",
        "        location = '/content/drive/MyDrive/Transliteration/' + name\n",
        "        model.save(location)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'method': 'grid',\n",
            " 'parameters': {'decoder_block': {'values': ['LSTM']},\n",
            "                'dropout': {'values': [0]},\n",
            "                'encoder_block': {'values': ['LSTM']},\n",
            "                'epochs': {'values': [60]},\n",
            "                'hidden_layer': {'values': [128]},\n",
            "                'latent_dimension': {'values': [256]}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1shGb7ZrJqH"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config, project=\"CS6910-assg3\")\n",
        "wandb.agent(sweep_id, training_sweep)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}